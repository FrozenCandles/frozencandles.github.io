<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>机器学习归档 - 冰封残烛的个人小站</title>
	<atom:link href="http://localhost/wordpress/archives/category/%E6%95%B0%E6%8D%AE%E7%A7%91%E5%AD%A6/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/feed?simply_static_page=3385" rel="self" type="application/rss+xml" />
	<link></link>
	<description>FrozenCandle&#039;s Personal Site</description>
	<lastBuildDate>Sat, 15 Apr 2023 09:30:45 +0000</lastBuildDate>
	<language>zh-CN</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.2.1</generator>

<image>
	<url>/wp-content/uploads/2022/02/cropped-preview-2-150x150.jpg</url>
	<title>机器学习归档 - 冰封残烛的个人小站</title>
	<link></link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>机器学习-参数选择与模型验证</title>
		<link>/archives/1081</link>
					<comments>/archives/1081#respond</comments>
		
		<dc:creator><![CDATA[Hello]]></dc:creator>
		<pubDate>Sat, 15 Apr 2023 09:29:37 +0000</pubDate>
				<category><![CDATA[机器学习]]></category>
		<category><![CDATA[Python]]></category>
		<guid isPermaLink="false">/?p=1081</guid>

					<description><![CDATA[<p>在很多时候并不能直接将数据以可视化的形式表达出来，这&#46;&#46;&#46;</p>
<p><a rel="nofollow" href="/archives/1081">机器学习-参数选择与模型验证</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></description>
										<content:encoded><![CDATA[
<p>在很多时候并不能直接将数据以可视化的形式表达出来，这就意味着无法以直观的方式检验模型的分类效果。本节介绍机器学习中常用的模型验证方法以及如何选择恰当的超参数。</p>

<h2>模型检验方法</h2>

<h3>交叉验证</h3>

<p>最简单验证模型的方法就是调用模型的 <code>.score()</code> 方法，检查给定数据的准确率。例如，对于以下数据：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-output-demo-data-1.png" alt="" width="380">
</figure>

<p>接下来建立一个决策树模型并用 <code>.score()</code> 方法检查训练数据的准确率：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>tree <span token="keyword">import</span> DecisionTreeClassifier</div><br><div>model <span style="color: #7c4dff;">=</span> DecisionTreeClassifier<span token="symbol">()</span></div><div>model<span token="symbol">.</span>fit<span token="symbol">(</span>X<span token="symbol">,</span> y_true<span token="symbol">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>DecisionTreeClassifier()</p></div></div>

    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model<span token="symbol">.</span>score<span token="symbol">(</span>X<span token="symbol">,</span> y_true<span token="symbol">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">1.0</div>
</div>

<p>结果显示模型的准确率是 100% 。当然，这种验证方法是不合理的，因为以上并没有对决策树模型做任何约束，此时可能已经发生了过拟合，因此可以 100% 拟合训练数据。</p>

<p>解决过拟合的一种思路是引入测试数据，测试数据不供模型学习，可以防止过拟合的影响。但是有时不易获取测试数据，这就需要将现有数据拆分为训练集和测试集：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>model_selection <span token="keyword">import</span> train_test_split </div><br><div>X_train<span token="symbol">,</span> X_test<span token="symbol">,</span> y_train<span token="symbol">,</span> y_test <span style="color: #7c4dff;">=</span> train_test_split<span token="symbol">(</span></div><div>&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; X<span token="symbol">,</span> y_true<span token="symbol">,</span> <span style="color: #e53935;">random_state</span><span style="color: #7c4dff;">=</span><span token="number">6</span><span token="symbol">,</span> <span style="color: #e53935;">train_size</span><span style="color: #7c4dff;">=</span><span token="number">0.7</span><span token="symbol">)</span></div></div>
    <div class="jupyter-separator"></div>
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model<span token="symbol">.</span>fit<span token="symbol">(</span>X_train<span token="symbol">,</span> y_train<span token="symbol">)</span></div><div>model<span token="symbol">.</span>score<span token="symbol">(</span>X_test<span token="symbol">,</span> y_test<span token="symbol">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.9142857142857143</div>
</div>

<p>以上函数将现有数据拆分为训练集和测试集，其中训练集占所有数据的 70% 。对测试集应用 <code>.score()</code> 方法的结果表明，模型的准确率为 91.4% ，更接近模型真实的准确率。</p>

<p>直接将数据分为训练集和测试集的缺点是模型失去了一部分可用于训练的数据，且这部分数据中可能包含某些关键的信息（比如支持向量）。一个更好的思路是将数据集划分为若干部分，每次将一个部分用于检验，其它部分用于训练，这样数据集的每个部分都能用于训练和检验。这种策略称为<strong>交叉验证</strong>(cross validation)。下图展示了一个 4 轮交叉验证的构成：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-cross-validation.png" alt="" width="320">
</figure>

<p>手动拆分数据比较繁琐，不过可以借助 Scikit-Learn 提供的工具来处理：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>model_selection <span token="keyword">import</span> cross_val_score</div><br><div>scores <span style="color: #7c4dff;">=</span> cross_val_score<span token="symbol">(</span>model<span token="symbol">,</span> X<span token="symbol">,</span> y_true<span token="symbol">,</span> <span style="color: #e53935;">cv</span><span style="color: #7c4dff;">=</span><span token="number">5</span><span token="symbol">)</span></div><div>scores</div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([0.975, 0.9  , 0.975, 0.875, 0.925])</div>
</div>

<p>可以通过 <code><em>cv</em></code> 参数修改交叉验证的轮数，默认采用 5 轮交叉验证。对每轮交叉验证的准确率取均值，就非常接近实际的准确率了：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>scores<span token="symbol">.</span>mean<span token="symbol">()</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.93</div>
</div>

<p>一种极端的情况是将交叉验证的轮数设置为和样本的个数一样多，这种情况下每次只使用一个样本用于验证，而其它的样本都用于训练，这称为留一(leave-one-out, LOO)交叉检验。要使用留一交叉验证不能直接将 <code><em>cv</em></code> 设置为样本的大小，而是需要借助 <code>LeaveOneOut</code> 类：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>model_selection <span token="keyword">import</span> LeaveOneOut</div><div>scores <span style="color: #7c4dff;">=</span> cross_val_score<span token="symbol">(</span>model<span token="symbol">,</span> X<span token="symbol">,</span> y_true<span token="symbol">,</span> <span style="color: #e53935;">cv</span><span style="color: #7c4dff;">=</span>LeaveOneOut<span token="symbol">())</span></div><div>scores<span token="symbol">.</span>mean<span token="symbol">()</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.925</div>
</div>

<p>在之前的学习中认识了模型的过拟合现象。过拟合现象就是随着模型的复杂度上升，虽然对数据的拟合效果变好，但是模型的泛化程度下降，对新的数据不能很好地处理。反应在交叉验证下，就是训练集效果越来越好，但是测试集效果反而越来越差：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-cross-validation-overfitting.png" alt="">
</figure>

<p>这个曲线同时表明训练集的准确率总是比验证集的准确率高，并且随着模型复杂度的提升，训练集的准确率越来越高，甚至趋近 100% ；而验证集的准确率增长到某个值后反而开始下降。</p>

<h3>混淆矩阵</h3>

<p>以上均使用准确率作为衡量模型好坏的指标，但这可能会导致一些问题。一个准确率较高的模型不一定效果就好。例如，对于以下数据：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-output-demo-data-2.png" alt="" width=350>
</figure>

<p>假设使用逻辑回归模型拟合这些数据，并且不对数据添加额外特征，那么得到模型的准确率为：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>linear_model <span token="keyword">import</span> LogisticRegression</div><div>model <span style="color: #7c4dff;">=</span> LogisticRegression<span token="symbol">().</span>fit<span token="symbol">(</span>X<span token="symbol">,</span> y_true<span token="symbol">)</span></div><div>cross_val_score<span token="symbol">(</span>model<span token="symbol">,</span> X<span token="symbol">,</span> y_true<span token="symbol">).</span>mean<span token="symbol">()</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.8181818181818183</div>
</div>

<p>82% 的准确率看似不错，但是注意到准确率会存在以下问题：对于二分类问题，如果两个分类样本数相近，那么即便模型随便预测，总体的准确率也接近 50% ；如果两个分类样本数相差较大，那么模型只要倾向于预测样本数较多的那个分类，准确率就不会差。例如对于以上数据，假设某个模型均预测为蓝色类别，那么它的准确率为：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>metrics <span token="keyword">import</span> accuracy_score</div><div>accuracy_score<span token="symbol">(</span>y_true<span token="symbol">,</span> np<span token="symbol">.</span>zeros_like<span token="symbol">(</span>y_true<span token="symbol">))</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.8909090909090909</div>
</div>

<p>结果甚至比逻辑回归还要好，但这种模型肯定是完全没有研究和使用价值的。</p>

<p>对于二分类模型，只有预测正确(True)和错误(False)的可能。记两种类别为正类(Positive)和负类(Negative)，那么一个模型的预测结果只有以下四种情况：</p>

<table>
    <tr>
        <th></th>
        <th>Positive（预测真）</th>
        <th>Negative（预测假）</th>
    </tr>
    <tr>
        <th>True（实际真）</th>
        <td>True Positive(TF) 正确肯定</td>
        <td>False Negative(FN) 漏报</td>
    </tr>
    <tr>
        <th>False（实际假）</th>
        <td>False Positive(FP) 虚报</td>
        <td>True Negative(TN) 正确否定</td>
    </tr>
</table>

<p>所有的评估参数都由该表中的元素组成，该表也被称为<strong>混淆矩阵</strong>(confusion matrix)。在之前一直使用准确(accuracy)率衡量模型，它的公式如下：</p>

<div class="math">
\\[
    \text{accuracy} = \frac{\text{True}}{\text{All}} = \frac{\text{TP}+\text{TN}}{\text{TP}+\text{FN}+\text{TN}+\text{FP}}
\\]
</div>

<p>这个公式的问题就在于它同时考虑了正类和反类，但两者的重要性是不一样的：样本多的类别对准确率的影响更大，但并不意味着模型应该优先满足样本多的类别的准确率。如果模型更关心正类的分类质量，那么有两个指标可以说明正类的分类效果好坏：</p>

<p>精确(precision)率：<strong>精确率</strong>用于衡量预测为正类的样本预测对的概率，即：</p>

<div class="math">
\\[
    \text{precision} = \frac{\text{True}}{\text{Predicted Positive}} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\\]
</div>

<p>如果模型的精确率高，说明预测的正类大多都预测对了。不过也不能一昧只提高精确率，那样的话模型为了确保预测的正类是对的，会倾向于减少预测为正类的样本数量。另一个指标<strong>召回率</strong>(recall)用于衡量实际为正类的样本预测对的概率，即：</p>

<div class="math">
\\[
    \text{recall} = \frac{\text{True}}{\text{Actual Positive}} = \frac{\text{TP}}{\text{TP}+\text{FP}}
\\]
</div>

<p>召回率高说明实际的正类大多都找出来了。同样，一昧提高召回率也会使得模型为了尽可能找出所有正类，倾向于增加预测为正类的样本数量。</p>

<p>容易看出，精确率和召回率之间具有互补关系：如果一个模型的精确率和召回率都很高，说明模型既能找出数据集中的正类，同时也能排除不属于正类的样本（找出所有的负类），那么这个模型的效果很好。通过组合精确率和召回率可以得到一个比较通用的指标，例如常用的 <span class="math">\\( F_1 \\)</span> 分数：</p>

<div class="math">
\\[
    F_1 = \frac{2}{ \displaystyle{\frac{1}{\text{precision}}} + \displaystyle{\frac{1}{\text{recall}}} }
\\]
</div>

<p><span class="math">\\( F_1 \\)</span> 分数使用调和平均数组合这两个指标。调和平均数是总体各统计变量倒数的算术平均数的倒数，由于不知道精确率和召回率哪个指标对总体的贡献更大，因此不能使用加权平均值，而调和平均数会赋予低值更高的权重，只有当召回率和精度都很高时才能得到较高的 <span class="math">\\( F_1 \\)</span> 分数。</p>

<p>在 Scikit-Learn 中，可以通过 <var type="module" class="code-font">metrics</var> 模块提供的工具计算 <span class="math">\\( F_1 \\)</span> 分数。</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>metrics <span token="keyword">import</span> f1_score</div><div>f1_score<span token="symbol">(</span>y_true<span token="symbol">,</span> model<span token="symbol">.</span>predict<span token="symbol">(</span>X<span token="symbol">))</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.4</div>
</div>

<p>计算得到模型的 <span class="math">\\( F_1 \\)</span> 分数只有 0.4 ，说明对正类的分类效果完全不像整体的准确率一样还算不错。</p>

<p>有时可能需要模型必须保证找到的正类都是对的，而允许模型存在一定的漏检（例如在登录时的用户面部识别，如果将错误的样本划归为正类会招致严重的损失）；有时可能需要确保模型没有漏检，而允许模型找到的正类不必都是对的（例如查找嫌疑人时的面部识别，如果遗漏一个正类会导致严重的后果）。这时可能会更关心精确率和召回率的其中一个，除了上文中见到的准确率计算，该模块也包含精确率和召回率的计算函数，可以组合它们拼凑一个需要的指标。</p>

<h3>评估曲线</h3>

<p>以上的准确率、精确度、<span class="math">\\( F_1 \\)</span> 值等都只是一个单一的数值指标，如果想观察分类算法在不同的参数下的表现情况，就可以使用一条曲线描述。</p>

<p>ROC(Receiver Operating Characteristic, 受试者工作特征)曲线是一种用于评估二元分类器性能的工具。它的横纵坐标都是来自混淆矩阵的组合指标：</p>

<ul>
    <li>横坐标为<strong>假正例率</strong>(False Positive Rate, FPR)，表示在所有真反例中，分类器错误识别为正例的比例 <span class="math">\\( \text{FPR} = \dfrac{\text{FP}}{\text{FP}+\text{TN}} \\)</span></li>
    <li>纵坐标为<strong>真正例率</strong>(True Positive Rate, TPR)，表示在所有真正例中，分类器正确识别为正例的比例 <span class="math">\\( \text{TPR} = \dfrac{\text{TP}}{\text{TP}+\text{FN}} \\)</span> ，就是刚才介绍的召回率</li>
</ul>

<p>ROC 曲线的特点是，它能够展示分类器在不同阈值下的性能表现。这里的阈值指的是分类器判断一个样本为正例的概率的截断值。一般情况下，当阈值设定为 0.5 时，分类器将所有概率大于 0.5 的样本判断为正例，而将所有概率小于等于 0.5 的样本判断为反例。</p>

<p>提高阈值会使预测为正类的样本数量减少，但预测的样本是正类的可能性都很高，因此可以提高模型的精确率。同样地，降低阈值会让模型将一些不太可能是正类的样本预测为正类，但也可能找出更多实际是正类的样本，因此可以提高模型的召回率。</p>

<p>ROC 曲线呈现了不同阈值下的假正例率和真正例率，如下图所示：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-ROC-curve-concept.png" alt="" width="320">
</figure>

<p>一个良好的模型应该拥有高真正例率(TPR)和低假正例率(FRP)，即尽可能找出属于该组的，但也要排除不属于该组的。一个完美的模型希望假警报率接近 0 ，命中率接近 1 ，即左上角顶点。一般的模型不可能达到该值，只能尽可能接近它，反应在图形上就是 ROC 曲线最接近左上角的一点，为模型的最优情况。</p>

<p>在 <span class="code-font">sklearn</span> 中，绝大多数分类器都有 <code>.predict_proba()</code> 方法，可以得到该模型判断样本为每个分类的概率：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model<span token="symbol">.</span>predict_proba<span token="symbol">(</span>X<span token="symbol">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([[0.0925382 , 0.9074618 ],
       [0.24097764, 0.75902236],
       [0.05459936, 0.94540064],
       ...,
       [0.20851233, 0.79148767]])</div>
</div>

<p>对于二分类模型，判断样本为正例或反例的概率加起来应该等于 1.0 。</p>

<p>有了这些概率数据，就可以通过设置不同阈值来观察假正例率和真正例率的变化。<span class="code-font">sklearn</span> 提供了如下工具来计算 <span class="math">\\( \text{TPR} \\)</span> 和 <span class="math">\\( \text{FPR} \\)</span> 在不同阈值下的表现情况：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>metrics <span token="keyword">import</span> roc_curve</div><div>FPR<span token="symbol">,</span> TPR<span token="symbol">,</span> thres <span style="color: #7c4dff;">=</span> roc_curve<span token="symbol">(</span>y_true<span token="symbol">,</span> </div><div>&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; model<span token="symbol">.</span>predict_proba<span token="symbol">(</span>X<span token="symbol">)[:,</span> <span token="number">1</span><span token="symbol">])</span></div></div>
</div>

<p>这里采用类别 1 作为正类。计算得到的结果是排序后的，可以直接调用 <span class="code-font">matplotlib</span> 绘制曲线，得到的结果可能是这样的：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-ROC-curve-demo.png" alt="">
</figure>

<p>在数值上通常使用 <strong>AUC</strong>(Area Under the ROC Curve, ROC 曲线下面积)作为衡量分类器性能的指标，AUC 取值范围为 <span class="math">\\( (0.5,1) \\)</span> ，越接近 1 表示分类器的性能越好。通常达到 0.75 表示可以接受，0.85 表示非常不错。</p>

<p>如果不是为了可视化，一般可以使用以下工具直接计算 AUC 值：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>metrics <span token="keyword">import</span> roc_auc_score</div><div>roc_auc_score<span token="symbol">(</span>y_true<span token="symbol">,</span> model<span token="symbol">.</span>predict_proba<span token="symbol">(</span>X<span token="symbol">)[:,</span> <span token="number">1</span><span token="symbol">])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.8382</div>
</div>

<p>进一步地，通过绘制 K-S 曲线可以分析模型随阈值变化的特性。K-S(Kolmogorov-Smirnov)检验用于测量两个累积分布是否一致。在二分类问题中，K-S 检验用于度量正分布和负分布之间的分离程度。</p>

<p>随着阈值的上升，真正例率(TPR)可以理解为累积正样本率，假正例率(FPR)可以理解为累积负样本率，因此可以使用这两个指标绘制 <strong>K-S 曲线</strong>。绘制时一般将阈值作为横坐标，真正例率(TPR)和假正例率(FPR)之差作为纵坐标，如下图所示：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-KS-curve-explain.png" alt="" width="410">
</figure>

<p>K-S 曲线有助于调整模型的决策阈值。K-S 值是两个两个累积分布曲线的最大垂直距离。在二分类问题中，它是曲线的纵坐标最大值。K-S 值的取值范围为 0~1 ，较高的值表示正类和负类之间的分离更好，即模型的分类效果更好。不过应该注意的是，K-S 检验假设数据是独立同分布的，在实践中可能未必如此。</p>

<h2>参数选择方法</h2>

<p>在了解了如何评价一个模型以及阈值对模型的影响后，接下来的问题是如何才能得到最优的模型，这就涉及到为模型选择合适的参数。</p>

<h3>鸢尾花数据集</h3>

<p>接下来介绍著名的鸢尾花数据集，它是一个经典的多分类数据，经常用于检验新模型的分类效果。它是 <span class="code-font">sklearn</span> 内置的数据集之一，可以通过以下方法加载它：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="keyword">from</span> sklearn<span token="symbol">.</span>datasets <span token="keyword">import</span> load_iris</div><div>iris <span style="color: #7c4dff;">=</span> load_iris<span token="symbol">()</span></div></div>
</div>

<p>得到的是一个类似字典的对象，它有以下键：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>iris<span token="symbol">.</span>keys<span token="symbol">()</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])</div>
</div>

<p>主要用到的键对应的值的含义为：</p>

<ul>
    <li><code>feature_names</code> ：特征名称</li>
    <li><code>data</code> ：特征数据，是一个二维数组对象</li>
    <li><code>target</code> ：目标数据</li>
    <li><code>target_names</code></li>
</ul>

<p>这些数据大致分布如下：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/04/06-iris-data.png" alt="" width="700">
</figure>

<p><code>iris['data']</code> 和 <code>iris['target']</code> 符合模型对特征矩阵 <code>X</code> 和目标数据 <code>y</code> 的要求，因此可以直接用于训练模型。</p>

<h3>网格搜索</h3>

<p>如果不确定某个参数要如何选择，需要多次调整参数并观察模型指标的变化，以此选择参数的最优值。而如果有多个参数需要选择，那么也需要调整参数的组合，并从中选择最优的结果。</p>

<p><strong>网格搜索</strong>是一种通过穷举搜索来选择最优参数组合的手段，它将使用所有候选参数的组合来循环建立模型，并选取表现最好的参数作为最终结果。</p>

<p>网格搜索的思路很简单，就是暴力穷举参数组合并观察效果。Scikit-Learn 提供了 <code>GridSearchCV</code> 工具来实现此过程的自动化。如果现在已经有了一个模型和一些候选参数：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>clf_tree <span style="color: #7c4dff;">=</span> DecisionTreeClassifier<span token="symbol">()</span></div><div>params_tree <span style="color: #7c4dff;">=</span> <span token="symbol">{</span></div><div>&#160; &#160; <span token="string">'min_samples_split'</span><span token="symbol">:</span> <span token="symbol">[</span><span token="number">2</span><span token="symbol">,</span> <span token="number">4</span><span token="symbol">,</span> <span token="number">6</span><span token="symbol">,</span> <span token="number">10</span><span token="symbol">],</span></div><div>&#160; &#160; <span token="string">'min_samples_leaf'</span><span token="symbol">:</span> <span token="symbol">[</span><span token="number">1</span><span token="symbol">,</span> <span token="number">3</span><span token="symbol">,</span> <span token="number">6</span><span token="symbol">,</span> <span token="number">10</span><span token="symbol">]</span></div><div><span token="symbol">}</span></div></div>
</div>

<p>那么首先需要创建一个网格搜索器：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>grid_search <span style="color: #7c4dff;">=</span> GridSearchCV<span token="symbol">(</span>clf_tree, params_tree, </div><div>&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;<span style="color: #e53935;">scoring</span><span style="color: #7c4dff;">=</span><span token="string">'f1_weighted'</span><span token="symbol">,</span> <span style="color: #e53935;">cv</span><span style="color: #7c4dff;">=</span><span token="number">5</span><span token="symbol">)</span></div></div>
</div>

<p>这样，<code>GridSearchCV</code> 会使用 5 轮交叉验证评估每个参数组合的效果，防止过拟合的影响。这里的评估标准 <code>"f1_weighted"</code> 指的是加权 F1 分数(Weighted F1-Score)，加权 F1 分数是对每个类别的 F1 分数进行加权平均，权重为各类别的样本数量占总样本数量的比例。关于 <span class="code-font">sklearn</span> 提供的更多检验标准，可以参考<a href="https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values" class="link-external">官方文档</a>。</p>

<p>有了评估器以后，就可以使用 <code>.fit()</code> 方法，表示<em>应用到数据</em>：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>grid_search.fit<span token="symbol">(</span>iris<span token="symbol">[</span><span token="string">'data'</span><span token="symbol">]</span>, iris<span token="symbol">[</span><span token="string">'target'</span><span token="symbol">])</span></div></div>
</div>

<p>应用数据之后，就可以获取最优的参数组合，例如：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span token="number">print</span>(grid_search.best_params_, grid_search.best_score_)</div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">{'min_samples_leaf': 1, 'min_samples_split': 6} 0.9664818612187034</div>
</div>

<p>如果只是想检查参数组合对模型分数的影响，也可以通过 <code>.cv_results_</code> 属性获取。它是一个很详细的数据，同时还记录了训练时间、验证时间等信息。</p>

<p><code>GridSearchCV</code> 的 <code><em>refit</em></code> 参数默认被设为 <code>True</code> ，这代表找到了最优的参数组合后，它将立刻被应用于训练模型，并且所有的数据都将作为训练集，这个完善的模型可以通过 <code>.best_estimator_</code> 获取，并直接参与 <code>.predict()</code> 的决策中。</p>

<p>最后要注意的是，如果这个模型是使用 <code>make_pipeline()</code> 与预处理结合得到的多个步骤，那么每个步骤的参数要用两个下划线区分，整个字典看起来类似这样：</p>

<div class="vscode-block"><div>params <span style="color: #7c4dff;">=</span> <span token="symbol">{</span></div><div>&#160; &#160; <span token="string">'logisticregression__penalty'</span><span token="symbol">:</span> <span token="symbol">[</span><span token="string">'l1'</span><span token="symbol">,</span> <span token="string">'l2'</span><span token="symbol">,</span> <span token="string">'none'</span><span token="symbol">],</span></div><div>&#160; &#160; <span token="string">'logisticregression__C'</span><span token="symbol">:</span> <span token="symbol">[</span><span token="number">0.2</span><span token="symbol">,</span> <span token="number">1.0</span><span token="symbol">,</span> <span token="number">4.0</span><span token="symbol">],</span> </div><div>&#160; &#160; <span token="string">'polynomialfeatures__degree'</span><span token="symbol">:</span> <span token="symbol">[</span><span token="number">3</span><span token="symbol">,</span> <span token="number">4</span><span token="symbol">,</span> <span token="number">5</span><span token="symbol">]</span></div><div><span token="symbol">}</span></div></div>

<h3>随机搜索</h3>

<p>网格搜索的缺点是如果参数列表很多，那么搜索范围会变得很大，再加上交叉验证会严重拖慢搜索进度。此时可以使用随机搜索 <code>RandomizedSearchCV</code> ，它只会挑选部分而不是全部的参数组合，再从这部分随机组合中挑选最优结果。这个类的用法与 <code>GridSearchCV</code> 大致相同。</p>

<p>随机搜索不仅速度更快，而且对是连续值的参数效果更好，因为随机搜索可以很方便地抽取大量参数用于检验，每个参数可能差别很小，完整训练的代价太大。</p>
<p><a rel="nofollow" href="/archives/1081">机器学习-参数选择与模型验证</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/archives/1081/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>机器学习-决策树</title>
		<link>/archives/1020</link>
					<comments>/archives/1020#respond</comments>
		
		<dc:creator><![CDATA[Hello]]></dc:creator>
		<pubDate>Wed, 08 Feb 2023 15:46:24 +0000</pubDate>
				<category><![CDATA[机器学习]]></category>
		<guid isPermaLink="false">/?p=1020</guid>

					<description><![CDATA[<p>决策树 决策树简介 和朴素贝叶斯一样，决策树(dec&#46;&#46;&#46;</p>
<p><a rel="nofollow" href="/archives/1020">机器学习-决策树</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></description>
										<content:encoded><![CDATA[
<h2>决策树</h2>

<h3>决策树简介</h3>

<p>和<a href="/archives/953">朴素贝叶斯</a>一样，决策树(decision tree)也是一种简单但有效的分类算法。决策树的原理是将数据的分布规律表达为一系列的条件判断，从而在引入新样本时通过一系列条件判断将其分到合理的类别中。这一系列条件判断会形成一个树形的结构，每个节点都是一次判断过程：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-decision-tree-concept.png" alt="" width="400">
    <figcaption>根据网图改的</figcaption>
</figure>

<p>接下来看一个实际的示例。对于以下数据：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-output-decision-step-0.png" alt="" width="340">
</figure>

<p>显然靠左侧位置分布的都是红色的样本，因此第一次可以将这些样本划分出来：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-output-decision-step-1.png" alt="" width="340">
</figure>

<p>而靠右侧位置分布的都是蓝色的样本，接下来将这些样本划分出来：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-output-decision-step-2.png" alt="" width="340">
</figure>

<p>对于中间位置的样本，靠上方的是蓝色位置的样本：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-output-decision-step-3.png" alt="" width="340">
</figure>

<p>靠下方的是红色位置的样本：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-output-decision-step-4.png" alt="" width="340">
</figure>

<p>这几个决策过程就构成了完整的分类依据。</p>

<p>在 Scikit-Learn 中，可以从以下位置导入决策树：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">tree </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> DecisionTreeClassifier</span></div><br><div><span style="color: #90a4ae;">model </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> DecisionTreeClassifier</span><span style="color: #39adb5;">().</span><span style="color: #90a4ae;">fit</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">X</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> y</span><span style="color: #39adb5;">)</span></div></div>
</div>

<p>下图展示了 Scikit-Learn 中决策树的分类边界：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-output-decision-step-std.png" alt="" width="420">
</figure>

<p>可以看到 Scikit-Learn 中的决策树似乎找到了一个更好的规律，使得只需要两次判断就能完成分类。不过相应的决策边界更靠近样本，过拟合的可能性增大。</p>

<p>既然一个数据集的决策树不是唯一的，那么就需要有一种指标来衡量哪些是好的决策树，或者说如何建立一棵决策树。决策树的建树依据主要是<strong>基尼系数</strong>(gini)和<strong>信息熵</strong>(Entropy)，它们的本质都是一样的，用于度量一个系统中的失序程序。建立决策树的思路就是使每一次决策后系统的混乱程度降低，从而得到有序的决策结果。</p>

<blockquote>
    <h4>什么是基尼系数</h4>

    <p>数据集 <span class="math">\\( D \\)</span> 的基尼系数 <span class="math">\\( \text{Gini}(D) \\)</span> 反映了从数据集 <span class="math">\\( D \\)</span> 中随机抽取两个样本，它们的类别不一致的概率。因此，如果数据有 <span class="math">\\( k \\)</span> 个类别，则基尼系数的计算公式为：</p>

    <div class="math">
    \\[
        \text{Gini}(D) = \sum_{i=1}^{k} p_i(1-p_i) = 1 - \sum_{i=1}^{k} p_i^2
    \\]
    </div>

    <p>基尼系数用于衡量一个系统的不确定性，系统的基尼系数越大，表示系统不确定性越高，系统的混乱程度越高，分类效果越差。</p>
    
    <p>接下来看一个具体示例。假设某个数据集中有 300 个数据，形成 3 个分类，每个分类数据个数相同，那么数据集的基尼系数为：</p>

    <div class="math">
    \\[
        \begin{align}
            \text{Gini}(D) &= 1 - (\frac 1 3)^2 - (\frac 1 3)^2 - (\frac 1 3)^2 \\
            &= \frac 2 3 \approx 0.67
        \end{align}
    \\]
    </div>

    <p>经过决策树的一次分类，将 <span class="math">\\( S_1=100 \\)</span> 个数据划分成一个子数据集，该数据集分别包含每个分类的样本 10 、20 、70 个，那么该子数据集的基尼系数为：</p>

    <div class="math">
        \\[
            \begin{align}
                \text{Gini}(D_1) &= 1 - (\frac 1 {10})^2 - (\frac 2 {10})^2 - (\frac 7 {10})^2 \\
                &= 0.46
            \end{align}
        \\]
    </div>

    <p>而另一部分子数据集的基尼系数为：</p>

    <div class="math">
        \\[
            \begin{align}
                \text{Gini}(D_2) &= 1 - (\frac 9 {20})^2 - (\frac 8 {20})^2 - (\frac 3 {20})^2 \\
                &= 0.615
            \end{align}
        \\]
    </div>

    <p>分类后，整个数据集的基尼系数需要考虑各自的样本量：</p>

    <div class="math">
    \\[
        \begin{align}
            \text{Gini}(D) &= \frac{S_1}{S_1 + S_2}\text{Gini}(D_1) + \frac{S_2}{S_1 + S_2}\text{Gini}(D_2) \\
            &\approx 0.563
        \end{align}
    \\]
    </div>

    <p>划分后，系统的基尼系数降低，说明该划分有效。从直观上看，每个子数据集的样本更容易确定属于哪个类别，样本的纯度更高。因此基尼系数也称<strong>基尼不纯度</strong>(gini impurity)</p>
    
    <p>如果经过若干次划分后，每个数据集中所有的样本都属于同一个类别，那么此时每个数据集的基尼系数为 0 ，总的基尼系数也为 0 ，得到了最好的划分结果。决策树的目的就是得到这样的结果。</p>
</blockquote>

<p>除了基尼系数外，还可以使用信息熵作为评估方式，它的含义是类似的。具体使用哪种评估方式通过超参数 <code><em>criterion</em></code> 设置，默认的标准是基尼系数。</p>

<p>决策树的一个特点是它天然支持多分类，而不需要使用一对多方法实现。下图展示了一棵多分类决策树的决策边界：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-tree-multi-classification.png" alt="" width="450">
</figure>

<p>对于连续数据，Scikit-Learn 使用<strong>分类和回归树</strong>(Classification and Regression Tree, CART)算法来建立一棵决策树，该算法找到某个特征变量合适的切分点，使得切分后左右两侧数据总体的基尼系数发生下降，那么说明这是一次合理的切分，可以作为一次决策过程。而左右两侧数据可以使用相同的逻辑递归切分，直到总的基尼系数降为 0 或达到停止条件为止。可以看出该算法仅生成二叉树，即它是一种 <code>if ... else</code> 形式的决策。而其它算法（例如 ID3 算法和它的改进版 C4.5 算法）可能找到多个切分点，形成类似 <code>elif</code> 的决策。</p>

<p>CART 是一种贪婪算法，因为每次切分只能得到一个局部较优解，但不能确定是否是全局最优解。而寻找全局最优解是一个很复杂的问题，当前的算法需要采用指数级的时间复杂度才能给出解，显然是不实际的。</p>

<p>CART 算法的这种性质还会带来另一个问题：由于每次被切分的维度是任意的，因此同一个模型多次调用决策树得到的决策边界往往不会完全相同。下图展示了相同的数据集使用不同随机种子生成的决策树，由于每次选择不同的维度划分数据而导致不同的结果：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-tree-param-randseed.png" alt="" width="500">
</figure>

<p>除此之外，由于 CART 算法每次都只从一个维度上切分，因此得到的决策边界由垂直于坐标轴的直线构成。如果数据集的边界是倾斜的，那么决策树只能通过锯齿状的边界来模拟它，这既会导致不必要的多次判断，也容易使决策树发生过拟合：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-output-tree-rotate.png" alt="" width="500">
    <figcaption>一个简单的数据集旋转后划分效果变得更糟</figcaption>
</figure>

<h3>正则化与决策树剪枝</h3>

<p>以上多分类的决策树看似表现良好，但注意到其中出现了一些不合常理的细长边界，这说明决策树可能已经出现了过拟合。</p>

<p>解决过拟合的方式是约束模型的某些参数不要过大，而约束决策树的方式是让它不要过度生长，导致决策太详细而出现过拟合。<strong>剪枝</strong>(pruning)是常见的约束方式，用于缩小决策树的规模，在防止决策树出现过拟合的同时还能提高分类速度。</p>

<p>决策树的剪枝方式主要分为以下两种：</p>

<ul>
    <li><strong>前剪枝</strong>(Pre-Pruning)</li>
</ul>

<p>前剪枝的思路是设置一个生长阈值，使决策树在生长完成前如果达到阈值就停止生长。</p>

<p>最简单的约束条件就是约束决策树的深度（根到树叶的路径长），在 Scikit-Learn 中由超参数 <code><em>max_depth</em></code> 控制。一旦达到最大深度，则决策树停止生长。</p>

<p>除了最大深度外，Scikit-Learn 还提供了一些其它前剪枝参数，如下：</p>

<table>
    <tr>
        <th>参数</th>
        <th>说明</th>
    </tr>
    <tr>
        <td class="code-font">min_samples_split</td>
        <td>节点继续划分（生长）所需的最小样本数，默认值为 2 ，因此只要可以都会继续划分</td>
    </tr>
    <tr>
        <td class="code-font">min_samples_leaf</td>
        <td>叶节点需要包含的最小样本数，默认值为 1 ，因此可能出现单个异常样本被划到一组的情况</td>
    </tr>
    <tr>
        <td class="code-font">max_leaf_nodes</td>
        <td>最大叶节点数，默认值 <code>None</code> 代表不限制，因此形成的决策边界可以无限复杂</td>
    </tr>
    <tr>
        <td class="code-font">min_impurity_decrease</td>
        <td>如果继续划分能使基尼不纯度的下降大于该值，那么便继续划分。默认值 0.0 代表只要能使不纯度下降就继续划分</td>
    </tr>
</table>

<p>可以看出决策树的默认参数并没有限制决策树的增长，适当增加最小值与约束最大值可以防止异常值的影响，并增强决策树的泛化能力。下图展示了一棵没有约束的决策树和加上不同约束条件后的决策边界：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-tree-param-pre-pruning.png" alt="" width="540">
</figure>

<ul>
    <li><strong>后剪枝</strong>(Post-Pruning)</li>
</ul>

<p>相比前剪枝，后剪枝的处理方式更复杂但也更有效。后的思路是先让决策树完全生长，再将其中不必要的分支去除（通常去除某个节点的子树或在结果中合并相近的类别）。后剪枝有许多算法，其中比较常用的是<strong>代价复杂度剪枝</strong>(Cost Complexity Pruning, CCP)。</p>

<p>剪枝用于平衡决策树的规模和正确率，代价复杂度剪枝中<em>代价</em>表示因剪枝而增加的错分样本，<em>复杂度</em>表示因剪枝而缩减的树的规模。代价复杂度剪枝使用以下公式衡量某个节点子树的代价复杂度：</p>

<div class="math">
\\[
    \alpha = \frac{R(t) - R(T)}{N(T)-1}
\\]
</div>

<p>其中 <span class="math">\\( R(T) \\)</span> 表示该节点未剪枝时的误差，<span class="math">\\( R(t) \\)</span> 表示剪枝后的误差，那么分母就表示由于剪枝增加的误差（代价）；<span class="math">\\( N(T) \\)</span> 表示该节点所有子树的树叶个数，那么分子可以反映子树的规模（复杂度）；因此 <span class="math">\\( \alpha \\)</span> 代表总体的代价复杂度，如果 <span class="math">\\( \alpha \\)</span> 很小，代表剪除某个节点的子树增加的误差不大且能去除较大规模的枝叶，那么这就是一次好的剪枝。</p>

<p>在实际剪枝中，一般先计算决策树每个非叶节点的 <span class="math">\\( \alpha \\)</span> ，并按 <span class="math">\\( \alpha \\)</span> 由小到大的顺序剪除对应节点的子树，直到设定的 <span class="math">\\( \alpha \\)</span> 阈值为止。也可以从每次剪枝都得到的决策树中选择最好的一棵。</p>

<!-- 
<blockquote>
    <figure>
        <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-CCP-example.png" alt="" width="390">
    </figure>

    <div class="math">
    \\[
        R(t) = r(t) \, p(t)
    \\]
    </div>

    <p>其中 <span class="math">\\( r(t) \\)</span> 为节点样本错误分类的比例，<span class="math">\\( p(t) \\)</span> 为节点包含样本占所有样本的比例。</p>

    <div class="math">
    \\[
        R(T) = \sum_i^{m}r_i(t) p_i(t)
    \\]
    </div>

    <p><code>#5</code> 节点的 <span class="math">\\( \alpha \\)</span></p> 
</blockquote> 
-->

<p>在 Scikit-Learn 中，可以通过 <code>.cost_complexity_pruning_path(<em>X</em>, <em>y</em>)</code> 方法获取决策树每一次 CCP 剪枝后的 <span class="math">\\( \alpha \\)</span> 值（按 <span class="math">\\( \alpha \\)</span> 由小到大剪枝）和不纯度：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">cost_complexity_pruning_path</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">X</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> y_true</span><span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">{'ccp_alphas': array([0.        , 0.00416667, 0.00485577, 0.00488095, ...]),
 'impurities': array([0.        , 0.00833333, 0.02775641, 0.03751832, ...])}</div>
</div>

<p>再次生成决策树时，通过 <code><em>ccp_alpha</em></code> 超参数可以控制决策树生长的阈值，这样就实现了后剪枝。</p>

<h3>决策树与模型可视化</h3>

<p>决策树的一个优点是每一步的决策过程都是透明的，如果有需要的话甚至可以检查决策树的每一步细节。</p>

<p>Scikit-Learn 提供了决策树与 Graphviz 交互的接口。Graphviz 是一个开源的可视化工具包，它可以绘制使用 <code>dot</code> 标记语言描述的包括流程图、关系图在内的各种图表。</p>

<p>可以在 https://graphviz.gitlab.io/download/ 里下载该软件。下载完成后安装到本地，安装时注意要将其添加到系统的环境变量中，这一步的目的是使用它提供的 dot 命令行工具完成文本到图片的编译。</p>

<p>可以使用 Scikit-Learn 提供的工具将模型导出为 <code>.dot</code> 文件：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">tree </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> export_graphviz</span></div><br><div><span style="color: #90a4ae;">export_graphviz</span><span style="color: #39adb5;">(</span></div><div><span style="color: #90a4ae;">&#160; &#160; model</span><span style="color: #39adb5;">,</span></div><div><span style="color: #90a4ae;">&#160; &#160; </span><span style="color: #e53935;">out_file</span><span style="color: #7c4dff;">=</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">tree.dot</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">,</span></div><div><span style="color: #90a4ae;">&#160; &#160; </span><span style="color: #e53935;">rounded</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">True</span></div><div><span style="color: #39adb5;">)</span></div></div>
</div>

<p>然后在命令行中使用 dot 工具完成图片的输出。可以输出的图片格式有许多种，包括常见的 PNG 、PDF 和 SVG 等：</p>

<div class="codeblock code-console">$ dot -Tpng tree.dot -o tree.png
</div>

<p>输出的决策过程如下：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-graphviz-export-tree.png" alt="" width="610">
</figure>

<p>其中 <code>X[0]</code> 表示样本的第一个特征变量（本例中为横坐标的值）；<code>gini</code> 表示该节点的基尼系数；<code>samples</code> 表示该节点包含的样本数；<code>value</code> 表示该节点中各类别样本的数量。根据这些提供的信息，就能清楚地了解到决策树运作的过程。下图是该决策树最终形成的决策边界：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-graphviz-source-tree.png" alt="" width="420">
</figure>

<h3>决策树回归</h3>

<p>决策树也可以用作回归。这种情况下，每个树叶节点不再预测离散的类别，而是一个连续的值，就像这样：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-tree-regression.png" alt="" width="440">
</figure>

<p>决策树回归与分类的原理是类似的，只不过不再采用基尼系数或熵作为划分的依据，而是均方误差 <span class="math">\\( \text{MSE} \\)</span> 。具体来说，就是在每一次分裂时试图使以下结果最小：</p>

<div class="math">
\\[
    \text{MSE} = \frac{n_{\text{left}}}{n} \text{MSE}_{\text{left}} + \frac{n_{\text{right}}}{n} \text{MSE}_{\text{right}}
\\]
</div>

<p><span class="math">\\( \text{MSE} \\)</span> 是节点中包含的所有样本真实值和预测值差值的和，预测值可以用所有样本的均值来描述。</p>

<p>决策树回归的优点是不用添加额外特征也能拟合曲线数据，但是拟合结果也不是曲线，而是阶跃的直线，并且和分类一样也有着严重的过拟合问题：如果不加约束地生长，决策的最终结果会落到每个值上：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/02/05-tree-regression-overfit.png" alt="" width="660">
</figure>

<p>可以用相同的方式对回归决策树剪枝，或者使用 Graphviz 导出为可视化图片。</p>

<p>总的来说，决策树是一种简单且直观的算法，它只求能够将两类样本分开，而不考虑分类的质量如何。除了容易导致过拟合外，决策树的分类边界也不够理想，不仅均垂直于坐标轴，而且完全没有考虑远离样本的问题，因此对异常值较为敏感。</p>

<p><a rel="nofollow" href="/archives/1020">机器学习-决策树</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/archives/1020/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>机器学习-支持向量机</title>
		<link>/archives/993</link>
					<comments>/archives/993#respond</comments>
		
		<dc:creator><![CDATA[Hello]]></dc:creator>
		<pubDate>Fri, 20 Jan 2023 03:41:31 +0000</pubDate>
				<category><![CDATA[机器学习]]></category>
		<guid isPermaLink="false">/?p=993</guid>

					<description><![CDATA[<p>支持向量机的概念 边界问题 在逻辑回归模型中介绍了决&#46;&#46;&#46;</p>
<p><a rel="nofollow" href="/archives/993">机器学习-支持向量机</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></description>
										<content:encoded><![CDATA[
<h2>支持向量机的概念</h2>

<h3>边界问题</h3>

<p>在<a href="/archives/966">逻辑回归模型</a>中介绍了决策边界的概念，一个好的模型决策边界要将两个类别完全区分开，例如以下这些决策边界都是合理的：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-about-boundary.png" alt="" width="250">
</figure>

<p>那么这就引出了一个问题，即如何在其中选择最佳的决策边界。在逻辑回归中，这条边界线的参数是通过求代价函数的最小值得到的。但直观上理解，一个好的决策边界应该离每个分类群体都尽可能远，这样在引入新的样本时不至于轻易地落到决策边界的另一边。</p>

<p><strong>支持向量机</strong>(support vector machine)使用了一种<em>边界(margin)最大化</em>的思路来解决这个问题，具体来说就是确定这样一条决策边界线，使它到两个分类的距离最远，那么它应该落在两个分类最中间的位置，因此这样的决策边界可以很好地分隔开两个分类，就像这样：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-boundary-margin.png" alt="" width="250">
</figure>

<h3>支持向量机的参数确定</h3>

<p>接下来介绍支持向量机的数学原理，这样可以明确模型到底需要求解什么。支持向量机的原理是让边界最大化，因此优化的最终目标就是求解最大的间距：</p>

<div class="math">
\\[
    \max \quad \text{margin} 
\\]
</div>

<p>这就需要从距离的角度建立模型。在平面中，点到一条直线的距离可以使用公式 <span class="math">\\( \displaystyle{d=\frac{|Ax_0+By_0+C|}{\sqrt{A^2+B^2} }} \\)</span> 计算。该公式可以推广到 3 维及更高维空间中，即点到 n 维度超平面的距离的计算公式为：</p>

<div class="math">
\\[
\begin{align}
    d &= \frac{|w_nx_n+\cdots+w_2x_2+w_1x_1+b|}{\sqrt{w_n^2+\cdots+w_2^2+w_1^2}}\\
      &= \frac{|\boldsymbol{w}^T\boldsymbol{x}+b|}{\|\boldsymbol{w}\|}
\end{align}
\\]
</div>

<p>在 SVM 中，需要确定这样一个直线或平面 <span class="math">\\( \boldsymbol{w}^T\boldsymbol{x}+b=0 \\)</span> ，使得它到两类样本的间距相同且最大，那么间距的计算公式为：</p>

<div class="math">
\\[
    \text{margin} = \underset{i}{\min} \frac{|\boldsymbol{w}^T\boldsymbol{x}_i+b|}{\|\boldsymbol{w}\|} > 0
\\]
</div>

<p>这里最小的意思是间距只取决于离决策边界最近点的计算结果。然后需要找出具有最大边界时的参数 <span class="math">\\( \boldsymbol{w}, b \\)</span> ，那么就需要求解以下目标：</p>

<div class="math">
\\[
    \underset{\boldsymbol{w}, b}{\mathrm{argmax}} \quad \text{margin} \quad \Rightarrow \quad
    \underset{\boldsymbol{w}, b}{\mathrm{argmax}}\frac{1}{\|\boldsymbol{w}\|}(\underset{i}{\min} |\boldsymbol{w}^T\boldsymbol{x}_i+b|)
\\]
</div>

<p>计算最小值时参数向量已经确定了，不影响最小值的计算结果，因此可以将其移到外面处理。因此 SVM 的优化目标为：</p>

<div class="math">
    \\[
        \underset{\boldsymbol{w}, b}{\mathrm{argmax}} \frac{1}{\|\boldsymbol{w}\|}
    \\]
</div>

<blockquote>
    <h4>SVM的优化目标是如何得到的</h4>

    <p>为了明白 SVM 的优化目标为什么是这个，需要弄清到底在求什么。求解的目标就是最大间距，但是间距有两个约束条件：</p>

    <ol>
        <li>所有样本都在间距外，即样本离决策边界最近的距离不小于间距</li>
        <li>决策平面需要落在两类样本之间，能将两类样本区分开</li>
    </ol>

    <p>因此有如下公式及约束条件：</p>

    <div class="math">
    \\[
    \begin{align}
        & \underset{\boldsymbol{w}, b}{\mathrm{argmax}} \quad d \\
        & \text{s.t.} \quad \frac{|\boldsymbol{w}^T\boldsymbol{x}_i+b|}{\|\boldsymbol{w}\|} \geq d > 0 \quad \text{and} \quad
        \begin{cases}
            \boldsymbol{w}^T\boldsymbol{x}_i+b > 0 \quad y_i = +1 \\
            \boldsymbol{w}^T\boldsymbol{x}_i+b < 0 \quad y_i = -1
        \end{cases}
    \end{align}
    \\]
    </div>

    <p>这里将两个分类记作 +1 和 -1，因为分类代表的数字只是一个记号而已，并且这样做可以将第二个约束条件改写为：</p>

    <div class="math">
    \\[
        y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) > 0
    \\]
    </div>

    <p>这样就可以将约束条件合并为一个：</p>

    <div class="math">
    \\[
        \text{s.t.} \quad \frac{y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)}{\|\boldsymbol{w}\|} \geq d
    \\]
    </div>

    <p>如果令 <span class="math">\\( k=d \|\boldsymbol{w} \| \\)</span> ，那么有：</p>

    <div class="math">
        \\[
        \begin{align}
            & \underset{\boldsymbol{w}, b}{\mathrm{argmax}} \quad \frac{k}{\|\boldsymbol{w}\|} \\
            & \text{s.t.} \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \geq k
        \end{align}
        \\]
    </div>

    <p>可以给参数 <span class="math">\\( \boldsymbol{w}, b \\)</span> 同时乘上 <span class="math">\\( k \\)</span> 从而消去该参数，这样就得到了以上的优化目标。而对参数 <span class="math">\\( \boldsymbol{w}, b \\)</span> 的同步放缩并不会改变优化目标，因为放缩后 <span class="math">\\( \displaystyle{k\boldsymbol{w}^T\boldsymbol{x}_i+kb = 0} \\)</span> 代表的是相同的决策边界，就像 <span class="math">\\( x+2y-3=0 \\)</span> 和 <span class="math">\\( 2x+4y-6=0 \\)</span> 代表的是同一条直线一样。</p>
</blockquote>

<p>就像代价函数一样，一般来说都将最大值问题转换为最小值问题分析，因此最终的目标就是解决以下问题：</p>

<div class="math">
\\[
    \begin{align}
        & \underset{\boldsymbol{w}, b}{\mathrm{argmin}} \, \|\boldsymbol{w}\| \\
        & \text{s.t.} \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \geq 1
    \end{align}
\\]
</div>

<p>解决带约束的最值问题一般使用的方法是拉格朗日乘子法。在<a href="#refrence">附录</a>中列出几篇文章，里面有更详细的证明。</p>

<h3>使用SVM</h3>

<p>在 Scikit-Learn 中，支持向量机模型位于 <code>svm</code> 模块中：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">svm </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> SVC</span></div></div>
</div>

<p><code>SVC</code> 即支持向量分类器(classifier)，如果要让它产生线性的决策边界，需要指定参数 <code><em>kernel</em></code> 为 <code>"linear"</code> ：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> SVC</span><span style="color: #39adb5;">(</span><span style="color: #e53935;">kernel</span><span style="color: #7c4dff;">=</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">linear</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">)</span></div><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">fit</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">data</span><span style="color: #39adb5;">[[</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">x</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">'</span><span style="color: #91b859;">y</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">]]</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> data</span><span style="color: #39adb5;">[</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">label</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>SVC(kernel='linear')</p></div></div>
</div>

<p>之后还会遇到非线性的决策边界。除了 <code>SVC</code> 外，Scikit-Learn 中还提供了 <code>LinearSVC</code> 类，它是专用于线性边界的支持向量分类器，并且带有正则化惩罚项，计算效率比 <code>SVC(<em>kernel</em>="linear")</code> 更快。</p>

<p>之前在介绍模型的原理时介绍过，间距是根据离决策边界最近的样本确定的，这些离决策边界最近的样本称为<strong>支持向量</strong>(support vectors)，所有的支持向量决定了支持向量机所具有的参数。至此可以明白支持向量机的名称由来，这些支持向量一起<em>支撑</em>起了分隔超平面的参数<em>向量</em>。不管模型的样本有多少，只要支持向量不变，那么模型便不发生改变：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-support-vectors.png" alt="" width="640">
    <figcaption>支持向量决定了模型的特点</figcaption>
</figure>

<p>在 Scikit-Learn 中，可以通过 <code>.support_vectors_</code> 参数得到建立模型的所有支持向量，或者通过 <code>.support_</code> 参数得到支持向量在样本中的索引：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">vects </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">support_vectors_</span></div><div><span style="color: #90a4ae;">vects</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([[4.11666667, 0.21666667],
       [2.61666667, 4.7       ],
       [6.18333333, 0.03333333]])</div>
</div>

<h2>支持向量机的注意事项</h2>

<p>支持向量机的原理较为简单，但在使用时也有一些问题需要注意。</p>

<h3>数据标准化</h3>

<p>支持向量机作为一种需要计算距离的模型，可能发生不同特征变量因为量纲级别相差过大导致的问题。为了更好地说明这个问题，以下是一个这样的示例：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-scale-affect.jpg" alt="" width="330">
</figure>

<p>直观上看，深绿色的边界应该能更好地区分两组样本，但计算结果却显示紫色是最好的边界。这是由于纵坐标的量纲明显远大于横坐标的量纲，对距离的影响权重也更大。因此在使用 SVM 之前，需要做<strong>数据标准化</strong>处理，主要目的是使不同特征的量纲级别统一，计算距离时拥有相同的权重。</p>

<p>常用的数据标准化方法有以下两个：</p>

<ol>
    <li>min-max 标准化</li>
</ol>

<p>min-max 标准化也称<strong>离差标准化</strong>，它利用原始数据的最大值和最小值把原始数据转换到 <span class="math">\\( [0,1] \\)</span> 区间内，具体的转换公式为：</p>

<div class="math">
\\[
    x^*=\frac {x-\min}{\max-\min}
\\]
</div>

<p>其中 <span class="math">\\( \max \\)</span> 和 <span class="math">\\( \min \\)</span> 分别为原始数据的最大值和最小值。</p>

<ol start="2">
    <li>z-score 标准化</li>
</ol>

<p>min-max 标准化处理方便，但有一个关键的缺点：如果数据中有远超正常数据取值范围的异常数据的话，那么会使得其它的正常数据被压缩在一个更小的区间内，反而不利于处理。z-score 标准化也称<strong>标准差标准化</strong>，通过原始数据的均值和标准差对数据执行标准化处理。</p>

<p>z-score 标准化后的数据符合标准正态分布，即均值为 0 ，标准差为 1 。它所使用的转换公式如下：</p>

<div class="math">
\\[
    x^* = \frac {x-\text{mean}}{\text{std}}
\\]
</div>

<p>其中 <span class="math">\\( \text{mean} \\)</span> 和 <span class="math">\\( \text{std} \\)</span> 分别为原始数据的均值和标准差。</p>

<p>使用代码对数据做标准化处理的步骤与为数据添加多项式特征的步骤一致：（ min-max 标准化也是一样的，只不过需要用到 <code>MinMaxScaler</code> 类）</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">preprocessing </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> StandardScaler</span></div><br><div><span style="color: #90a4ae;">scalar </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> StandardScaler</span><span style="color: #39adb5;">()</span></div><div><span style="color: #90a4ae;">X_standard </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> scalar</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">fit_transform</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">X</span><span style="color: #39adb5;">)</span></div></div>
</div>

<p>经过标准化处理得到的数据可以被 SVM 更好地划分了：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-standard-scaled-1.png" alt="" width="360">
</figure>

<h3>软间隔</h3>

<p>以上介绍的支持向量机要求两个类别能够被直线分开（线性可分），但有的时候样本未必能被直线完全分开，或者分开的效果并不好。</p>

<p>下图展示了两种这样的情况：左图中由于一个异常样本使得决策边界被大幅改变，而右图中的异常点使得根本不存在这样一个边界能将两类样本完美区分：</p>

<figure class="parallel">
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-worse-situation.png" alt="" width="340">
        <figcaption>异常样本使边界被不合理地改变</figcaption>
    </div>
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-worst-situation.png" alt="" width="370">
        <figcaption>异常样本使边界不存在</figcaption>
    </div>
</figure>

<p>相比让间隔完全分开两个类别（硬间隔），适当允许一些样本存在间隔中甚至另一边可以缓解由异常值导致的问题。这种情况下，模型的间隔不再要求一定要硬性分隔两个类别，因此称为软间隔。</p>

<p>为了衡量一个样本与正常分类间隔的偏离程度，需要引入了松弛变量 <span class="math">\\( \xi \\)</span> 。一个样本离它应在的区域越远，则对应的 <span class="math">\\( \xi \\)</span> 也就越大：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-soft-margin-xi.jpg" alt="" width="360">
</figure>

<p>引入松弛变量后，模型的参数需要重新确定，软间隔 SVM 的基本数学形式如下：</p>

<div class="math">
    \\[
        \begin{align}
            & \underset{\boldsymbol{w}, b, \xi}{\mathrm{argmin}} \, \|\boldsymbol{w}\| + C \sum_{i=1}^{n}\xi_i \\
            & \text{s.t.} \quad y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \geq 1 - \xi_i , \quad \xi_i \geq 0
        \end{align}
    \\]
</div>

<p>参数 <span class="math">\\( C \\)</span> 用于控制模型的软硬程度：越大的 <span class="math">\\( C \\)</span> 值会使模型的约束更大，<span class="math">\\( \xi_i \\)</span> 也需要更小一些，样本不能超过间隔太多；若 <span class="math">\\( C \\)</span> 为无穷大，则 <span class="math">\\( \xi_i \\)</span> 只能无限接近于 0 ，样本只能在间隔的两边，此时 SVM 变回了硬间隔 SVM 。</p>

<p>在 Scikit-Learn 中，同名超参数 <code><em>C</em></code> 用于控制模型的软硬程度：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-parameter-C.jpg" alt="" width="600">
    <figcaption>超参数 C 用于控制模型的软硬程度</figcaption>
</figure>

<p>通过设置一定的 <code><em>C</em></code> 值，可以使模型更加灵活，避免异常数据带来的不利影响。</p>

<h3>核化SVM</h3>

<p>之前介绍的 SVM 都具有线性边界，但有些时候样本并不能用直线分开，例如：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-poly-boundary-1.png" alt="" width="400">
</figure>

<p>即使将模型设置得足够软，使用直线也无法完全分开两个类别。这时，可以使用在<a href="/archives/966">逻辑回归</a>中提及的方法，为模型添加上非线性特征项，例如多项式特征项：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> make_pipeline</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">PolynomialFeatures</span><span style="color: #39adb5;">(</span><span style="color: #e53935;">degree</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">2</span><span style="color: #39adb5;">),</span></div><div><span style="color: #90a4ae;">&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; StandardScaler</span><span style="color: #39adb5;">(),</span><span style="color: #90a4ae;"> </span></div><div><span style="color: #90a4ae;">&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; SVC</span><span style="color: #39adb5;">(</span><span style="color: #e53935;">C</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">6</span><span style="color: #39adb5;">))</span></div></div>
</div>

<p>此时模型的划分效果为：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-poly-boundary-3.jpg" alt="" width="400">
</figure>

<p>添加非线性特征实现简单，但如果添加的特征较多，会在计算 <span class="math">\\( \boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_j \\)</span>（参见附录 SVM 的求解）向量内积时速度变慢。</p>

<p>因此这种变换并不是任意的，通过特定的<strong>核函数</strong>(kernel function)可以在为数据添加许多特征后，并没有引入额外特征的计算。具体而言，就是找出这样的函数 <span class="math">\\( k(\boldsymbol{x}, \boldsymbol{y}) = \phi(\boldsymbol{x})^\mathrm{T}\phi(\boldsymbol{y}) \\)</span> ，使得数据由 <span class="math">\\( \boldsymbol{x} \\)</span> 变为 <span class="math">\\( \phi(\boldsymbol{x}) \\)</span> 后，计算 <span class="math">\\( \phi(\boldsymbol{x})^\mathrm{T}\phi(\boldsymbol{y}) \\)</span> 可以化简为计算 <span class="math">\\( \boldsymbol{x}^\mathrm{T}\boldsymbol{y} \\)</span> ，从而使复杂度相同或相近。在使用时只需要定义具体的核函数，这样就不需要定义映射函数 <span class="math">\\( \phi(\boldsymbol{x}) \\)</span> ，从而只需要计算变换后的向量内积，省去了变换过程的开销。</p>

<p>在之前的线性 SVM 中，使用参数 <code><em>kernel</em>="linear"</code> 表示使用线性核函数 <span class="math">\\( k(\boldsymbol{x}, \boldsymbol{y}) = \boldsymbol{x}^\mathrm{T} \boldsymbol{y} \\)</span> ，即不添加任何特征。线性核函数计算速度快，且还存在许多优化算法（例如 <code>LinearSVC</code> 使用的算法），是初次处理数据的首选算法。</p>

<p>除了线性核函数，对于之前的非线性数据可以使用多项式核函数 <span class="math">\\( k(\boldsymbol{x}, \boldsymbol{y}) = (\gamma\boldsymbol{x}^\mathrm{T} \boldsymbol{y} + c)^d \\)</span> 。多项式核函数的特点是可以通过使用不同的 <span class="math">\\( d \\)</span> 控制最高项的次数，从而控制特征映射的复杂度。但一般不建议使用过高的次数，因为这可能使核函数的结果太大或太小。</p>

<p>在 <code>SVC</code> 中，通过指定超参数 <code><em>kernel</em></code> 为 <code>"poly"</code> 可以使用多项式核函数。如果使用多项式核函数，通过超参数 <code><em>degree</em></code> 可以控制最高项的次数（默认为三次）；通过超参数 <code><em>gamma</em></code> 可以控制同名系数；通过超参数 <code><em>coef0</em></code> 可以控制常数项。例如，以下是最高项次数为二次的多项式 <code>SVC</code> 分类边界：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-poly-boundary-2.png" alt="" width="400">
</figure>

<p>要注意的是，为数据引入非线性特征的目的并不是为了得到非线性的分隔面，因为点到曲面的距离难以求得。引入非线性特征的实质是将数据变换到高维空间中，使得变换后的数据在高维空间中线性可分。</p>

<p>为了更好地说明这个问题，接下来看一组示例数据：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-kernelize-explain-1.jpg" alt="" width="400">
</figure>

<p>对于这样的数据，可以采用径向基函数核(Radial Basis Function, RBF，也称高斯核) <span class="math">\\( k(\boldsymbol{x}, \boldsymbol{y}) = \exp(-\gamma\|\boldsymbol{x}-\boldsymbol{y}\|^2) \\)</span> 将其变换到高维空间。高斯核函数的特点是可以映射到无限维的空间中，因此具有很强的表示能力，但是相应的计算速度更慢，也更容易出现过拟合现象。</p>

<p>通过指定超参数 <code><em>kernel</em></code> 为 <code>"rbf"</code> 可以使用高斯核函数，这也是 <code>SVC</code> 默认采用的核函数。采用高斯核函数将原有数据映射到三维空间中，便可以在三维空间中看到数据可以由平面分隔开了：</p>

<figure class="parallel">
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-kernelize-explain-2.jpg" alt="" width="370">
        <figcaption>将数据用径向基函数核映射到三维使其线性可分</figcaption>
    </div>
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-kernelize-explain-3.png" alt="" width="370">
        <figcaption>一个更好的视角说明两类样本是如何被线性划分的</figcaption>
    </div>
</figure>

<p>对于更高维的情况也是同理，因为实际数据的维度总是有限的，那么就肯定存在更高维的映射，使数据在更高维的空间中可以被超平面合理地分开。</p>

<p>高斯核函数的另一个优点是可供调节的参数相比多项式核函数更少。唯一的参数 <span class="math">\\( \gamma \\)</span> 用于控制高斯核的平滑程度：越大的 <span class="math">\\( \gamma \\)</span> 将会使高斯核函数变化更剧烈，虽然能更好的拟合数据但会使过拟合风险增大。这对应于 <code>SVC</code> 的 <code><em>gamma</em></code> 参数，下图展示了不同参数取值情况下的分类边界：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-parameter-gamma.jpg" alt="" width="680">
</figure>

<h3>支持向量回归</h3>

<p>支持向量机不仅可以用作分类，还可以用于回归。在用作分类任务时，要求支持向量机规划一条最好的分类边界，使得它尽可能分开两类样本。而用作回归分析时，工作原理则相反：这时要求支持向量机规划一条最好的拟合边界，使得它尽可能包括所有的样本，就像这样：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-svr-concept.png" alt="" width="400">
</figure>

<p>但通过包括所有样本来确定回归直线显然是不合理的，因为受到异常数据的影响很大。为了防止异常值的影响，要求边界不需要包含所有的样本，而是让所有样本都不要偏移边界内太多。</p>

<p>假设分隔面与两边界面的常数项相差 <span class="math">\\( \epsilon \\)</span> ，那么一个样本与边界的偏离程度可以使用以下公式衡量：</p>

<div class="math">
\\[
    \xi_i = \max(|\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+b-y|-\epsilon, 0)
\\]
</div>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-svr-cost.png" alt="" width="310">
</figure>

<p>如果样本位于边界以内，那么就不算偏离。总的代价函数每一个样本偏离程度的和，再包括正则项：</p>

<div class="math">
    \\[
        \underset{\boldsymbol{w}, b}{\mathrm{argmin}} \quad C \sum_{i=1}^{n} \max(|\boldsymbol{w}^\mathrm{T}\boldsymbol{x}_i+b-y|-\epsilon, 0) + \frac 1 2 \|\boldsymbol{w}\|^2
    \\]
</div>

<p>包含正则项后的 SVR 代价函数和 SVC 的代价函数较为相似，可以使用类似的方式求解。</p>

<p>在 Scikit-Learn 中，<code>SVR</code> 和 <code>LinearSVR</code> 是支持向量机的回归版本，后者针对线性模型做了额外的优化。从上文介绍中可以看出，它们有两个较为重要的超参数：</p>

<ul>
    <li><code><em>epsilon</em></code> ：分隔面与两边界的距离（实际上不是距离，而是常数项的差值）</li>
</ul>

<p>由于落在两边界之内的样本不算偏离，对代价函数没有作用，因此 SVR 的支持向量是落在边界之外的样本，所有边界之外的样本才决定了边界的参数。参数 <code><em>epsilon</em></code> 可以看作模型对偏离数据的容忍程度，越大的参数值将会使边界内包含更偏离的数据，但这些不好的数据也会影响回归直线，使其对数据的拟合程度下降。</p>

<p>下图展示了不同 <code><em>epsilon</em></code> 取值下对数据的拟合程度：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-svr-parameter-epsilon.jpg" alt="" width="610">
</figure>

<p>因此一般来说应选用尽量小的 <code><em>epsilon</em></code> 值。<code>SVR</code> 给出的默认值为 0.1 ；而 <code>LinearSVR</code> 给出的默认值为 0.0 ，这时所有数据的偏离程度都需要考虑，支持向量回归退化为和线型回归很相似的模型了。</p>

<ul>
    <li><code><em>C</em></code> ：模型的正则化强度。注意公式中的 <span class="math">\\( C \\)</span> 不是正则项的系数，因此它的值越大正则化程度越低</li>
</ul>

<p>下图展示了不同 <code><em>C</em></code> 取值下对数据的拟合程度：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2023/01/04-output-svr-parameter-C.jpg" alt="" width="610">
</figure>

<p>取值过小时正则化太强，容易发生欠拟合；反之容易发生过拟合。</p>

<p>总的来说，支持向量机是一种可解释性强的分类模型，并可以用作回归。尽管没有用到统计方法，但它有严格的数学理论支撑，在实践中表现良好。支持向量机只取决于是它的少数支持向量，因此占用内存较少，预测新样本的速度也较快。核函数的引入使其不但可以处理各种类型的非线性数据，也可以很好地处理高维数据。</p>

<p>不过支持向量机也有训练时间长，对参数选择依赖较大等缺点，因此一般只在小批量样本上使用它。</p>

<h2 id="appendix">附录</h2>

<h3>SVM求解</h3>

<p>本节简单介绍 SVM 目标问题的求解方法。在<a href="#refrence">参考文献</a>中列出几篇文章，里面有更详细的证明。</p>

<p>首先回顾优化目标：</p>

<div class="math">
\\[
    \begin{align}
        & \underset{\boldsymbol{w}, b}{\mathrm{min}} \, \frac 1 2 \boldsymbol{w}^\mathrm{T}\boldsymbol{w} \\
        & \text{s.t.} \quad 1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b) \leq 0
    \end{align}
\\]
</div>

<p>这是一个有约束条件的最值问题，一般使用拉格朗日乘子法处理这种问题，拉格朗日函数如下：</p>

<div class="math">
\\[
    L(\boldsymbol{w}, b, \lambda) = \frac 1 2 \boldsymbol{w}^\mathrm{T}\boldsymbol{w} + \sum_{i=1}^{n} \lambda_i(1 - y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))
\\]
</div>

<p>问题转换为求解 <span class="math">\\( \underset{\boldsymbol{w}, b}{\min} \underset{\lambda}{\max} L(\boldsymbol{w}, b, \lambda) \\)</span> 。由于 SVM 满足 KKT 条件，因此它的对偶问题与原问题等价，也就是说可以交换里外的求最值顺序：</p>

<div class="math">
\\[
    \underset{\boldsymbol{w}, b}{\min} \underset{\lambda}{\max} L(\boldsymbol{w}, b, \lambda)
    \Rightarrow
    \underset{\lambda}{\max} \underset{\boldsymbol{w}, b}{\min} L(\boldsymbol{w}, b, \lambda)
\\]
</div>

<p>交换后内侧的最小值问题是对 <span class="math">\\( \boldsymbol{w}, b \\)</span> 的无约束优化问题，可以通过令偏导为零的方式得到：</p>

<div class="math">
\\[
    \begin{align}
        \frac {\partial L}{\partial w} &= 0 \Rightarrow w = \sum_{i=1}^{n} \lambda_i y_i \boldsymbol{x_i} \\
        \frac {\partial L}{\partial b} &= 0 \Rightarrow 0 = \sum_{i=1}^{n} \lambda_i y_i
    \end{align}
\\]
</div>

<p>这时将求解 <span class="math">\\( \boldsymbol{w}, b \\)</span> 的过程转换成与 <span class="math">\\( \lambda \\)</span> 相关的问题，虽然没有得到 <span class="math">\\( b \\)</span> 和 <span class="math">\\( \lambda \\)</span> 的关系，但是可以将该等于作为约束项求解问题。</p>

<p>接下来将结果代入拉格朗日函数中，化简可得优化问题的最终形式：</p>

<div class="math">
\\[
    \begin{align}
        & \underset{\lambda}{\max} \quad L(\boldsymbol{w}, b, \lambda) = \sum_{i=1}^{n}\lambda_i - \frac 1 2 \sum_{i=1}^{n} \sum_{j=1}^{n} \lambda_i \lambda_j y_i y_j \boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_j \\
        & \text{s.t.} \quad \sum_{i=1}^{n} \lambda_i y_i = 0 \quad \text{and} \quad \lambda_i \geq 0
    \end{align}
\\]
</div>

<p>得到最终结果后，只需将 <span class="math">\\( \boldsymbol{x} \\)</span> 代入，即可得到有关 <span class="math">\\( \lambda_i \\)</span> 的公式。要得到最大的结果，只需要对每个 <span class="math">\\( \lambda_i \\)</span> 求偏导，并令结果等于 0 ，借助约束条件 <span class="math">\\( \displaystyle{\sum_{i=1}^{n} \lambda_i y_i = 0} \\)</span> 就可以得到合适的 <span class="math">\\( \lambda_i \\)</span> 值。从公式中也可以看出，主要的计算量在 <span class="math">\\( \boldsymbol{x}_i^\mathrm{T}\boldsymbol{x}_j \\)</span> 上。</p>

<p>根据得到的 <span class="math">\\( \lambda_i \\)</span> ，代入之前公式就可以计算 <span class="math">\\( \boldsymbol{w} \\)</span> ，而再利用一个支持向量就可以计算参数 <span class="math">\\( b \\)</span> 。</p>

<h3 id="refrence">参考资料/延伸阅读</h3>

<p>
    <a href="https://zhuanlan.zhihu.com/p/480302399">https://zhuanlan.zhihu.com/p/480302399</a><br>
    一份非常非常详细的 SVM 从零推导过程
</p>

<p>
    <a href="https://zhuanlan.zhihu.com/p/24638007">https://zhuanlan.zhihu.com/p/24638007</a><br>
    一篇不错的关于 SVM 原理推导的文章
</p>
<p><a rel="nofollow" href="/archives/993">机器学习-支持向量机</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/archives/993/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>机器学习-逻辑回归</title>
		<link>/archives/966</link>
					<comments>/archives/966#respond</comments>
		
		<dc:creator><![CDATA[Hello]]></dc:creator>
		<pubDate>Fri, 23 Dec 2022 02:51:10 +0000</pubDate>
				<category><![CDATA[机器学习]]></category>
		<guid isPermaLink="false">/?p=966</guid>

					<description><![CDATA[<p>分类算法 上一节介绍了线性回归，线性回归是一种经典的&#46;&#46;&#46;</p>
<p><a rel="nofollow" href="/archives/966">机器学习-逻辑回归</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></description>
										<content:encoded><![CDATA[
<h2>分类算法</h2>

<p><a href="/archives/880">上一节</a>介绍了线性回归，线性回归是一种经典的<em>回归</em>算法，它可以根据一系列自变量来预测因变量的值。</p>

<p><strong>分类</strong>(classification)也是一种常见的需求。分类问题并不要求预测具体的值，而是需要判断样本属于的类别。例如分析一株植物属于哪个物种，或者判断一封邮件是否是垃圾邮件等。分类问题的因变量是离散的，因此无法通过简单的计算阐述结果。</p>

<p>二分类是比较常见的一种分类情况，二分类下每个样本只有两种分类情况（例如某件事是否会发生），因此可以使用布尔值 0 和 1 来描述分类。类似地有多分类，每个样本有多种分类情况。</p>

<p>有许多经典的算法都可以处理分类问题，本节介绍最简单的分类算法之一：逻辑回归。</p>

<h2>逻辑回归</h2>

<h3>逻辑回归的概念</h3>

<p>首先先看一个典型的分类场景。假设有如下的一系列数据，横坐标代表样本某个特征的值，纵坐标代表样本的分类，并且是离散的：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-logistic-intro-1.png" alt="" width="260">
</figure>

<p>在以上示例中，可以很明显看到分类为“ 0 ”的样本特征变量的值都小于 300 ，而分类为“ 1 ”的样本特征变量的都大于 300 。那么就可以用特征变量的值是否大于 300 作为样本分类的依据。</p>

<p>但在稍微复杂一点的场景中，这种简单的逻辑就会失效。例如，考虑以下数据：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-logistic-intro-2.png" alt="" width="260">
</figure>

<p>在 300 附近，样本可能属于分类“ 0 ”也可能属于分类“ 1 ”。但是数据仍然具有这样的规律：当特征变量的值小于 300 较多时，该样本属于分类“ 0 ”；当特征变量的值大于 300 较多时，它属于分类“ 1 ”；但特征变量的值在 300 附近时，它同时可能属于其中的任何一个分类。</p>

<p>那么此时就无法直接确定某些范围样本的分类了。相比起死板地直接分为两类（硬分类），可以使用另一种思路解决这个问题：确定样本属于分类的概率（软分类）。显然，远大于 300 的样本属于分类“ 1 ”的概率更大，远小于 300 的样本属于分类“ 0 ”的概率更大，而接近 300 的样本属于两个样本的概率相差不大。</p>

<p>这就是<strong>逻辑回归</strong>(logistic regression)的基本思路。逻辑回归通过建立一条 <span class="math">\\( y \\)</span> 值分布在 0~1 之间的函数来计算样本的分类概率：<span class="math">\\( y \\)</span> 值表示样本属于分类“ 1 ”的概率，那么 <span class="math">\\( 1-y \\)</span> 就表示样本属于分类“ 0 ”的概率，从而将分类问题转换为回归问题。最终样本的分类也容易确定：如果 <span class="math">\\( y > 0.5 \\)</span> ，那么说明样本更可能属于分类“ 1 ”，反之说明样本更可能属于分类“ 0 ”。</p>

<p>可以使用线性回归拟合一条范围在 0~1 内的直线作为概率函数，类似这样：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-logistic-intro-3.png" alt="" width="260">
</figure>

<p>但是直线比较僵硬，不但不能表示在分界区域概率密度的变化过程，还要在两端将其约束到 0~1 的范围内。一般来说，逻辑回归最常用的概率函数为 sigmoid 函数，这个函数的表达式为：</p>

<div class="math">
\\[
    f(y) = \frac{1}{1 + e^{-y}}
\\]
</div>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-sigmoid-function.png" alt="" width="380">
</figure>

<p>该函数的特点是当 <span class="math">\\( y \\)</span> 离 0 较远时，<span class="math">\\( f(y) \\)</span> 值十分趋近 0 或 1 ；而当 <span class="math">\\( y \\)</span> 接近 0 时，函数由 0 到 1 有一个较为平缓的过渡。并且该函数是中心对称的，因此比较贴近真实情况下的概率分布。</p>

<blockquote>
    <p>逻辑回归使用 sigmoid 函数是经过严格推导后得到的结果，这个 <a href="https://www.quora.com/Logistic-Regression-Why-sigmoid-function/answer/Roi-Yehoshua?target_type=answer">Quora 上的回答</a>从概率分布的角度解释了为什么。</p>
</blockquote>

<h3>决策边界</h3>

<p>注意到 sigmoid 函数在 <span class="math">\\( y=0 \\)</span> 处取得概率 0.5 ，而在两端概率分别小于 0.5 和大于 0.5 ，因此可以说 <span class="math">\\( y=0 \\)</span> 是一个<strong>决策边界</strong>(decision boundary)，位于边界两端的样本将会被分到不同的类别。</p>

<p>在实际应用中，<span class="math">\\( y=0 \\)</span> 并不一定就是区分两个类别的边界条件，而且特征变量的个数也不止一个。这个时候可以使用换元法将自变量变换到实际模型中，由实际情况下的特征变量及其参数组合确定：</p>

<div class="math">
\\[
    y = \theta_n x^n + \theta_{n-1} x^{n-1} + \cdots + \theta_1 x + \theta_0
\\]
</div>

<p>那么实际情况下的概率 <span class="math">\\( h_\theta(y) \\)</span> 将由以下公式确定：</p>

<div class="math">
\\[
\begin{align}
    h_\theta(y) &= \frac{1}{1 + {\large e}^{\displaystyle{-(\theta_n x^n + \theta_{n-1} x^{n-1} + \cdots + \theta_1 x + \theta_0)}}}\\
                &= \frac{1}{1 + {\large e}^{\displaystyle{-\boldsymbol{\theta}^T \boldsymbol{x}}}}
\end{align}
\\]
</div>

<p>此时 <span class="math">\\( \boldsymbol{\theta}^T \boldsymbol{x} = 0 \\)</span> 就是实际情况的一个决策边界：当 <span class="math">\\( \boldsymbol{\theta}^T \boldsymbol{x} > 0 \\)</span> 时，可以预测样本属于类别“ 1 ”；当 <span class="math">\\( \boldsymbol{\theta}^T \boldsymbol{x} < 0 \\)</span> 时，则可以预测样本属于类别“ 0 ”。</p>

<p>通过取用合适的参数 <span class="math">\\( \boldsymbol{\theta} \\)</span> ，可以得到不同的决策边界，从而适应不同的实际情况。例如，对于以下具有两个特征的数据集：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-decision-boundary-intro-1.png" alt="" width="260">
</figure>

<p>可以看出，在这个二维平面上可以作出这样一条直线 <span class="math">\\( \theta_2x_2+\theta_1x_1+\theta_0=0 \\)</span> ，将两种类别的样本分隔在直线两侧，就像这样：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-decision-boundary-intro-2.png" alt="" width="260">
</figure>

<p>但有时候边界可能比较复杂，并不能用直线描述，例如：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-decision-boundary-intro-3.png" alt="" width="260">
</figure>

<p>那么这个时候，可以像在线性回归中处理非线性数据一样，通过引入非线性特征项，在平面上使用曲线例如二次多项式曲线 <span class="math">\\( \theta_2x_2+\theta_{12}x_1^2+\theta_{11}x_1+\theta_0=0 \\)</span> 作为决策边界，将两种类别的样本分隔在曲线两侧，就像这样：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-decision-boundary-intro-4.png" alt="" width="260">
</figure>

<p>这也就说明了线性回归和逻辑回归之间的联系：线性回归主要是拟合实际的数据，而逻辑回归主要是拟合实际的决策边界。逻辑回归模型可以通过设计复杂的曲线，来表达复杂的决策边界，从而适应各种复杂的模型。</p>

<h2>代价函数</h2>

<h3>代价函数的概念</h3>

<p>以上基本已经介绍了逻辑回归模型的基本概念，但是目前还有一个问题没有解决，那就是参数 <span class="math">\\( \boldsymbol{\theta} \\)</span> 还没有确定。</p>

<p>在线性回归中，确定参数的方式是让拟合的直线最贴近原始的数据，使预测值和真实值的差异（即<em>残差</em>）平方和最小。之所以要附加一个平方，是因为残差可正可负，通过平方可以消除符号带来的影响。</p>

<p>使用残差平方和，可以建立一个具体的指标来描述数据和模型的拟合程度：当残差平方和最小，说明预测值和真实值的差异最小，回归直线的拟合更好。并且这个指标是具体的，易于通过程序计算最小值。</p>

<p>这种衡量模型与实际样本误差指标的函数称为成本函数或<strong>代价函数</strong>(cost function)，当代价函数取值最小时，说明模型与实际值的误差就最小，那么模型拟合效果就最好。</p>

<p>为了确定最好的逻辑回归模型，也需要一个类似的代价函数去描述它的误差。类似线性回归，同样可以使用预测值和真实值的差值作为样本的误差，只不过预测值是连续的概率，而真实值是连续的类型。这样，每个样本的代价函数定义为：</p>

<div class="math">
\\[
    \mathrm{cost}(h_\theta (\boldsymbol{x}), y) = \begin{cases}
        -\log(h_\theta (\boldsymbol{x})) & \text{if} \quad y = 1 \\
        -\log(1 - h_\theta (\boldsymbol{x})) & \text{if} \quad y = 0
    \end{cases}
\\]
</div>

<p>逻辑回归计算的概率是相对类型“ 1 ”而言的，因此对于类型“ 0 ”的概率要用 1 减去该值。取对数的原因是为了让概率接近 1（与实际分类结果接近）时，误差将变得很小；而概率接近 0（完全分错类了）时，误差将变得非常大，从而防止为了让边界上的一些样本取得更大的概率错误地挪动边界，而将某个样本置于明显错误的位置的情况。最后考虑到误差越小，代价函数也应该越小，再给对数结果取相反值。</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-cost-function-why-log.png" alt="" width="440">
    <figcaption>为什么要取对数：严格的计算表明不取对数时会使用不好的决策边界</figcaption>
</figure>

<p>最终整个模型的代价函数是每个样本代价函数的和，有时再除去样本个数：</p>

<div class="math">
\\[
    J(\boldsymbol{\theta}) = \frac 1 n \sum_{i=1}^{n} \mathrm{cost}(h_\theta (\boldsymbol{x_i}), y_i)
\\]
</div>

<p>为了让模型最贴近实际样本，需要让代价函数最小，也就是让代价函数的导数等于 0 。但是没有直接得出结果的公式（闭式解），因此需要别的方法来得出最小值。下面介绍一个使用最广泛的方法：梯度下降法。</p>

<h3>梯度下降</h3>

<p>假设一个人被困在遍布密林的深山中，无法判断当前所处的位置，那么快速下山的一种策略就是沿着最陡的方向下坡。这就是<strong>梯度下降</strong>(gradient descent)法的基本原理：通过计算当前参数 <span class="math">\\( \boldsymbol{\theta} \\)</span> 下代价函数的局部梯度，并不断沿着降低梯度的方向调整参数 <span class="math">\\( \boldsymbol{\theta} \\)</span> ，直到梯度降为 0 ，说明代价函数取得极小值：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-gradient-descent.png" alt="" width="440">
    <figcaption>图片来源于网络</figcaption>
</figure>

<p>第一个 <span class="math">\\( \boldsymbol{\theta} \\)</span> 值可以随机选择，每次执行梯度下降都会使代价函数降低一些。最终收敛得到一个极小值：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-gradient-descent-steps.png" alt="" width="300">
</figure>

<p>不同的梯度下，参数沿着能让代价函数下降方向改变的步长也不一样：如果梯度较平缓，那么步长应该小一些，以免错过了极小值。步长的改变量取决于<strong>学习率</strong>(learning rate)，学习率越高则参数的改变程度也越大，具体由以下公式确定：</p>

<div class="math">
    \\[
        \theta_j \Leftarrow \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)
    \\]
</div>

<p>可以看出，学习率对梯度下降结果很重要：低下的学习率将会使算法需要太多次迭代才能达到极值点，而过高的学习率则可能使算法直接越过极值点到达对坡面，甚至在这一过程中发散而永远无法到达极值点：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-gradient-descent-steps-bad-1.png" alt="" width="540">
</figure>

<p>并且梯度下降得到的是极值点，但未必是最值点，下图展示了这种情况：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-gradient-descent-steps-bad-2.png" alt="" width="320">
</figure>

<p>这就要求代价函数尽量是一个凸(convex)函数，它的极值就是最值。凸函数也便于自动计算合适的学习率。可以证明逻辑回归的代价函数是一个凸函数，它求偏导的结果如下：（省略过程）</p>

<div class="math">
\\[
    \frac{\partial}{\partial \theta_j}J(\theta)=\frac 1 n \sum_{i=1}^{n}[h_\theta(x_i)-y_i]x_{i(j)}
\\]
</div>

<p>因此逻辑回归可以很好地使用梯度下降算法得到具有最小误差的最好模型。下图展示了一个逻辑回归模型是如何从最开始的随机参数，经由梯度下降收敛到最优参数的：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-output-gradient-descent-steps.png" alt="" width="380">
</figure>

<p>总的来说，梯度下降是一种非常通用的优化算法，适用于很多找最优解的问题中。梯度下降也有许多改进版本，进一步拓宽了它的适用范围。</p>

<h3>正则化下的逻辑回归模型</h3>

<p>在线性回归模型中介绍过在拟合现有数据时可能因为设置太高次项而导致的过拟合问题，在逻辑回归中也可能存在这种情况。例如，以下是某个逻辑回归模型中因为异常数据而带来的过拟合：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-logistic-overfitting.jpg" alt="" width="390">
</figure>

<p>这个决策边界显然是不合理的。可以像线性回归一样给代价函数加上正则化约束项，从而防止决策边界取得过大的参数而过于复杂。</p>

<p>具体来说，常用的正则化项有如下两个：</p>

<ol>
    <li>L1 正则化项：<span class="math">\\( \displaystyle{\frac{\alpha}{2n}\sum_{j=0}^{n}\theta_j^2} \\)</span></li>
    <li>L2 正则化项：<span class="math">\\( \displaystyle{\frac{\alpha}{2n}\sum_{j=0}^{n}|\theta_j|} \\)</span></li>
</ol>

<p>这实际上就是线性回归中介绍的岭回归和 LASSO 回归使用的正则化项。以下是使用正则化项后的模型，可以看到过拟合的情况明显改善了很多：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/02-logistic-overfitting-prevent.jpg" alt="" width="390">
</figure>

<h3>Scikit-Learn中的逻辑回归模型</h3>

<p>最后简单介绍使用 Scikit-Learn 建立逻辑回归模型的方法，它和线型回归模型遵循同样的接口，例如建立模型：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">linear_model </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> LogisticRegression</span></div><br><div><span style="color: #90a4ae;">model </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> LogisticRegression</span><span style="color: #39adb5;">()</span></div><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">fit</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">X</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> y</span><span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>LogisticRegression()</p></div></div>
</div>

<p>以及对新的样本作出预测，由于逻辑回归是解决分类问题，因此预测得到的是一个离散的结果：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">predict</span><span style="color: #39adb5;">([[</span><span style="color: #f76d47;">270</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">295</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">305</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">320</span><span style="color: #39adb5;">]])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([0, 0, 1, 1])</div>
</div>

<p>逻辑回归的特点就是使用概率确定所属的分类，在代码中可以使用 <code>.predict_proba()</code> 方法来获取这个概率：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">predict_proba</span><span style="color: #39adb5;">([[</span><span style="color: #f76d47;">270</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">297</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">300</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">320</span><span style="color: #39adb5;">]])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([[9.99411784e-01, 5.88216310e-04],
       [6.59217705e-01, 3.40782295e-01],
       [4.76691144e-01, 5.23308856e-01],
       [5.97572205e-03, 9.94024278e-01]])</div>
</div>

<p>相比线性回归，逻辑回归模型多了很多超参数，其中许多参数都涉及本文介绍的内容，接下来介绍其中的一部分：</p>

<div class="codeblock code-template">
LogisticRegression(<br>
&nbsp; &nbsp; <em>penalty</em>="l2",<br>
&nbsp; &nbsp; <em>l1_ratio</em>=None,<br>
&nbsp; &nbsp; <em>C</em>=1.0,<br>
&nbsp; &nbsp; <em>tol</em>=1e-4,<br>
&nbsp; &nbsp; <em>max_iter</em>=100,<br>
&nbsp; &nbsp; <em>verbose</em>=0,<br>
&nbsp; &nbsp; <em>multi_class</em>="auto"<br>
)
</div>

<p>参数 <code><em>penalty</em></code> 指定使用的正则化项，如果设置为 <code>"none"</code> 代表不使用正则化。除了前面介绍的 <code>"l1"</code> 和 <code>"l2"</code> 两种正则化之外，还可以使用一种称为“弹性网络” <code>"elasticnet"</code> 的正则化项，它是 L1 和 L2 的混合形式，参数 <code><em>l1_ratio</em></code> 就控制着这种混合比例。参数 <code><em>C</em></code> 是大于 0 的正则化强度，但注意它不是系数，在这里值越小代表正则化效果越强。</p>

<p>前文介绍了逻辑回归使用梯度下降的方式优化系数。但浮点运算并不是完全精确的，往往不会出现导数正好为 0 的情况。参数 <code><em>tol</em></code> 决定了它的收敛条件：如果相邻两次计算的步长小于这个值，说明结果已经足够精确，可以不继续算下去。如果达到了 <code><em>max_iter</em></code> 代表的最大迭代次数还是无法找到这样一个收敛条件，那么说明模型可能存在问题，则停止继续计算。</p>

<p><code><em>verbose</em></code> 是一个较为通用的参数，启用后会输出一些运算过程信息，并且数字越大，信息也越多。</p>

<p>以上介绍的逻辑回归只能处理二分类问题，但无法解决多个分类的情况，原因在于所使用的概率函数不能同时描述多个分类的概率。如果尝试使用 Scikit-Learn 中的逻辑回归模型处理多分类问题，会发现它仍然可以正常工作并得到较为正确的结果：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">X </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> np</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">array</span><span style="color: #39adb5;">([</span><span style="color: #f76d47;">267</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">273</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">278</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">319</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">325</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">333</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">435</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">447</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">456</span><span style="color: #39adb5;">]).</span><span style="color: #90a4ae;">reshape</span><span style="color: #39adb5;">(</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">)</span></div><div><span style="color: #90a4ae;">y </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> np</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">array</span><span style="color: #39adb5;">([</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">2</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">2</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">2</span><span style="color: #39adb5;">])</span></div><div><span style="color: #90a4ae;">model </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> LogisticRegression</span><span style="color: #39adb5;">().</span><span style="color: #90a4ae;">fit</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">X</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> y</span><span style="color: #39adb5;">)</span></div><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">predict</span><span style="color: #39adb5;">([[</span><span style="color: #f76d47;">270</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">320</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">440</span><span style="color: #39adb5;">]])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([0, 1, 2])</div>
</div>

<p>参数 <code><em>multi_class</em></code> 就是为了解决多分类问题而设置的。接下来介绍如何使用逻辑回归处理多分类问题。</p>

<h2>多分类下的逻辑回归</h2>

<h3>一对多方法</h3>

<p>为了实现多分类，可以使用一种简单的变换方法：将多分类模型转换为多个二分类模型。具体来说，假设有 A 、B 、C 三个分类，那么可以使用以下三个模型描述它：</p>

<ol>
    <li>属于(1)和不属于(0)分类 A</li>
    <li>属于(1)和不属于(0)分类 B</li>
    <li>属于(1)和不属于(0)分类 C</li>
</ol>

<p>可以单独建立这几个二分类模型，对于每个模型都将另外两类看成一个整体，然后组合它们得到的结果：如果在模型 1 中“属于(1)”的分数比模型 2 和 3 的“属于(1)”都高，那么说明更可能属于分类 A 。</p>

<p>这等价于参数 <code><em>multi_class</em>="ovr"</code> 。</p>

<h3>softmax回归</h3>

<p>还有一种方法也可以处理多分类情况：既然之前使用的概率函数不能同时描述多个分类的概率，那么也可以更换一个概率函数。具体说来，可以将概率函数转换为更贴近概率定义的分数形式：</p>

<div class="math">
\\[
    P(y=k)=\frac{\displaystyle{{\Large e}^{\displaystyle{s_k(\boldsymbol{x})}}}}{\displaystyle{\sum_{j=1}^{K} {\Large e}^{\displaystyle{s_j(\boldsymbol{x})}}}}
\\]
</div>

<p>这种形式的逻辑回归称为 softmax 回归，它本质上是逻辑回归的推广形式。对于每一个类别，softmax 回归都使用不同的参数去表示它的概率形式：</p>

<div class="math">
\\[
    s_k(\boldsymbol{x}) = \boldsymbol{x}^T\boldsymbol{\theta}_k
\\]
</div>

<p>这等价于参数 <code><em>multi_class</em>="multinomial"</code> 。但很明显，这种新的概率函数不能再用之前的代价函数去描述它的误差了，并且新的代价函数也不能像之前的函数那么定义：概率最多就为 1 ，根据无法表达与类别“ 2 ”的误差。一般来说，softmax 回归使用交叉熵作为它的代价函数，其公式如下：</p>

<div class="math">
\\[
    J(\boldsymbol{\Theta}) = - \frac 1 n \sum_{i=1}^{n}\sum_{k=1}^{K} y_k^{(i)}\log (P(y^{(i)} = k))
\\]
</div>

<p>注意这里使用参数矩阵 <span class="math">\\( \boldsymbol{\Theta} \\)</span> 来存储每个类别的参数向量 <span class="math">\\( \boldsymbol{\theta} \\)</span> 。</p>

<!--如果想知道为什么 softmax 回归这样定义以及为什么使用交叉熵作为代价函数，请参阅相关资料。-->

<p>总的来说，逻辑回归作为一种简单的分类算法，模型简单且数学上易于分析，具有运算速度快，对预处理要求少，可以得出概率便于进一步分析等优点。不过逻辑回归的实际分类准确率并不高，特别是可能无法处理一个分类形成多个群落的情况，因此一般只用于处理简单的分类问题。</p>
<p><a rel="nofollow" href="/archives/966">机器学习-逻辑回归</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/archives/966/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>机器学习-朴素贝叶斯</title>
		<link>/archives/953</link>
					<comments>/archives/953#respond</comments>
		
		<dc:creator><![CDATA[Hello]]></dc:creator>
		<pubDate>Fri, 16 Dec 2022 15:00:20 +0000</pubDate>
				<category><![CDATA[机器学习]]></category>
		<guid isPermaLink="false">/?p=953</guid>

					<description><![CDATA[<p>贝叶斯分类器 算法原理 贝叶斯分类源自概率论上著名的&#46;&#46;&#46;</p>
<p><a rel="nofollow" href="/archives/953">机器学习-朴素贝叶斯</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></description>
										<content:encoded><![CDATA[
<h2>贝叶斯分类器</h2>

<h3>算法原理</h3>

<p>贝叶斯分类源自概率论上著名的贝叶斯定理(Bayes's theorem)。设某事件的样本空间为 <span class="math">\\( S \\)</span> ，事件 <span class="math">\\( A \\)</span> 与 <span class="math">\\( B \\)</span> 为 <span class="math">\\( S \\)</span> 中的两个事件，则为事件 <span class="math">\\( B \\)</span> 发生的条件下事件 <span class="math">\\( A \\)</span> 发生的概率可以使用如下的贝叶斯公式计算：</p>

<div class="math">
\\[
    P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\\]
</div>

<p>该公式可以结合以下 Venn 图理解，注意 <span class="math">\\( P(B|A) = \displaystyle{\frac{P(A \cap B)}{P(A)}} \\)</span> ：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/03-prob-Venn.png" alt="" width="280">
</figure>

<p>贝叶斯公式的用途为：很多时候并不能直接得出在事件 <span class="math">\\( B \\)</span> 发生的情况下事件 <span class="math">\\( A \\)</span> 发生的概率，但是根据贝叶斯公式，可以采用一种迂回的方式计算出它的概率。</p>

<p>接下来看一个示例。假设研究人员统计了 10000 份邮件，其中包含 1500 份垃圾邮件，发现垃圾邮件中有 30% 的邮件都包含单词“ sale ”，而非垃圾邮件中只有 1% 包含该单词。如果现在新收到了一份邮件，它包含该单词，问它是垃圾邮件的概率。</p>

<!-- 
<div class="math">
    \\[
        P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
    \\]
</div> 
-->

<p>这种问题直接分析较难，但借助贝叶斯公式可以根据已有条件计算。设事件 <span class="math">\\( X1 \\)</span> 为一封邮件是垃圾邮件，<span class="math">\\( X2 \\)</span> 为是普通邮件，<span class="math">\\( Y \\)</span> 为包含该关键词的邮件，则 <span class="math">\\( X1 \cup X2=S , X1 \cap X2= \varnothing \\)</span> ，那么有：</p>

<div class="math">
\\[
P(X_1) = 0.15, P(X_2) = 0.85, P(Y|X_1) = 0.3, P(Y|X_2) = 0.01
\\]
</div>

<p>由贝叶斯公式，可以完成如下计算：</p>

<div class="math">
    \\[
        P(X_1|Y) = \frac{P(Y|X_1)P(X_1)}{P(Y)} = \frac{P(Y|X_1)P(X_1)}{P(X_1)P(Y|X_1)+P(X_2)P(Y|X_2)} = \frac{90}{107} \approx 84.11 \%
    \\]
</div>

<p>结果显示这封邮件有很高的概率是垃圾邮件，那么它更应该被归入垃圾邮件的分类中。</p>

<p>从计算过程中可以看到，虽然直接分析包含“ sale ”单词的邮件是垃圾邮件的概率较难，但是邮件中垃圾邮件和非垃圾邮件的概率是很方便统计的，而两种邮件的词频也是很好统计的，这些根据以往样本就可以计算得到的概率称为<strong>先验概率</strong>(prior probability)；而与之相对的<strong>后验概率</strong>(posterior probability)表示某个已经发生的事件应该发生的概率。容易看出，贝叶斯公式是一种用先验概率计算后验概率的公式。</p>

<!-- <p>如果计算出在满足 <span class="math">\\( X=1 \\)</span> 中因变量 <span class="math">\\( Y=1 \\)</span> 的情况明显集中，则可以说明该条件是分类为 1 的一个特征。</p> -->

<h3>朴素贝叶斯</h3>

<p>在实际问题中，问题的特征变量往往不止一个。不过贝叶斯公式也可以推广至 <span class="math">\\( n \\)</span> 个特征变量，如果某个发生的事件其中每个特征变量的取值情形为 <span class="math">\\( X_1, X_2, \dots, X_n \\)</span> 时，那么贝叶斯公式可以做如下推广：</p>

<div class="math">
\\[
    P(Y|X_1, X_2, \dots, X_n) = \frac{P(X_1, X_2, \dots, X_n | Y) \cdot P(Y)}{P(X_1, X_2, \dots, X_n)}
\\]
</div>

<p>它的本质和单个特征变量的贝叶斯公式是一致的。</p>

<p>朴素(naive)贝叶斯模型则假设各个特征变量间相互独立，互不影响，那么该组合概率可以拆分成多个独立概率的积，即可以对上式做如下简化：</p>

<div class="math">
\\[
    P(X_1, X_2, \dots, X_n) = P(X_1) \cdot P(X_2) \cdots P(X_n)
\\]
</div>

<div class="math">
\\[
    P(X_1, X_2, \dots, X_n|Y) = P(X_1 | Y) \cdot P(X_2 | Y) \cdots P(X_n | Y)
\\]
</div>

<p>这样就将一个复杂的组合概率拆分成多个简单概率的运算。这些简单的概率都易于统计，那么根据贝叶斯公式，就可以计算在 <span class="math">\\( n \\)</span> 个特征变量取不同值的情况下，目标变量为某个分类的概率，并将样本归入概率更高的分类中。</p>

<p>稍后会介绍朴素贝叶斯的应用。</p>

<h3>高斯朴素贝叶斯</h3>

<p>贝叶斯公式只能计算离散数据的概率，但对连续数据的分类比较困难。例如，对于以下在平面中分布的二维数据：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/03-output-gaussian-situation.png" alt="" width="420">
    <figcaption>不同的颜色描述样本的不同分类</figcaption>
</figure>

<p>无法用传统的方式来描述样本的概率。不过如果可以假设数据服从高斯分布（正态分布），且各个特征变量间线性无关，那么就可以使用正态分布的概率公式来描述它：</p>

<div class="math">
    \\[
        P(X) = \frac{1}{\sqrt{2 \pi}\sigma}{\large{e}}^{\displaystyle{-\frac{(x-\mu)^2}{2\sigma ^2}}}
    \\]
</div>

<p>每个分类包含样本的均值和标准差很容易计算，代入公式就可以得出先验概率。下图展示了每一类样本的概率密度以及类别间的决策边界：</p>

<figure class="parallel">
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/03-gaussian-probability.png" alt="" width="340">
        <figcaption>高斯朴素贝叶斯的概率密度，椭圆表示 3-sigma 边界</figcaption>
    </div>
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/03-output-gaussian-border.png" alt="" width="400">
        <figcaption>由贝叶斯公式得到的决策边界</figcaption>
    </div>
</figure>

<p>高斯朴素贝叶斯在 Scikit-Learn 中的实现遵循相同的接口，并且贝叶斯模型几乎没有什么可以调整的参数：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">naive_bayes </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> GaussianNB</span></div><div><span style="color: #90a4ae;">model </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> GaussianNB</span><span style="color: #39adb5;">()</span></div><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">fit</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">X</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> y_true</span><span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>GaussianNB()</p></div></div>
</div>

<p>当训练完高斯朴素贝叶斯模型后，可以通过 <code>.theta_</code> 和 <code>.var_</code> 属性检查每个类别每个特征变量的均值和方差：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">theta_</span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span><span style="color: #90a4ae;"> &#160;</span><span style="color: #a8a8a8;"># mu (mean)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([0.82083046, 3.94121607])</div>

    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">var_</span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span><span style="color: #90a4ae;"> &#160; </span><span style="color: #a8a8a8;"># sigma ** 2 (variance)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([2.05995969, 2.92921668])</div>
</div>

<p>贝叶斯分类的一个特点是可以直接得出样本属于每一类的概率（概率分类）而不是只给出分类结果，这样有助于判断落在分类边界上的样本。预测分类概率可以通过 <code>.predict_proba()</code> 方法实现：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">predict_proba</span><span style="color: #39adb5;">([[</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">2</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">2</span><span style="color: #39adb5;">],</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #f76d47;">5</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #f76d47;">4</span><span style="color: #39adb5;">]])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([[4.23140192e-01, 5.76859808e-01, 5.44257518e-14],
       [9.02345446e-01, 6.10995956e-13, 9.76545538e-02]])</div>
</div>

<p>接下来通过一个具体的示例说明朴素贝叶斯分类的应用。</p>

<h2>示例：垃圾邮件检测</h2>

<p>本节将使用朴素贝叶斯算法将邮件分为普通邮件和垃圾邮件，所使用的数据集可以在 <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/00228/">http://archive.ics.uci.edu/ml/machine-learning-databases/00228/</a> 处下载。由于这里只有英文的数据，因此只能用作英文邮件分类。（中文邮件的分类思路也是一致的，只不过需要更复杂的预处理而已）</p>

<p>这些数据的结构如下：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">mail </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> pd</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">read_table</span><span style="color: #39adb5;">(</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">dataset/SMSSpamCollection</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #e53935;">names</span><span style="color: #7c4dff;">=</span><span style="color: #39adb5;">[</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">label</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">'</span><span style="color: #91b859;">text</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">])</span></div><div><span style="color: #90a4ae;">mail</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">head</span><span style="color: #39adb5;">(</span><span style="color: #f76d47;">3</span><span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><table class="pandas-dataframe">
        <tr>
            <th></th><th>label</th><th>text</th>
        </tr>
        <tr>
            <td>0</td><td>ham</td><td>Go until jurong point, crazy.. Available only ...</td>
        </tr>
        <tr>
            <td>1</td><td>ham</td><td>Ok lar... Joking wif u oni...</td>
        </tr>
        <tr>
            <td>2</td><td>spam</td><td>Free entry in 2 a wkly comp to win FA Cup fina...</td>
        </tr>
    </table></div>
</div>

<p>在介绍数据具体的应用之前，先介绍数据处理的方法。</p>

<h3>数据预处理：词袋模型</h3>

<p>在使用朴素贝叶斯模型来判断一封邮件是否是垃圾邮件前，需要提取自变量的特征，以确定公式的具体组成。</p>

<p>本次提取的文本特征为单词出现的频率，并使用词频计算贝叶斯模型中的概率。可以使用<strong>词袋模型</strong>(bag-of-words)来处理文本。词袋模型是一种基于词频的文本表示方法，它忽略了词的顺序，只考虑词在文本中出现的次数。词袋模型将所有样本中出现过的特征词汇表汇总成一个向量，每个词汇对应一个维度。</p>

<p>例如，对于以下的文本数据：（示例来自<a href="https://en.wikipedia.org/wiki/Bag-of-words_model">维基百科</a>）</p>

<div class="vscode-block"><div><span style="color: #90a4ae;">texts </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> </span><span style="color: #39adb5;">[</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">John likes to watch movies. Mary likes movies too.</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">,</span></div><div><span style="color: #90a4ae;">&#160; &#160; &#160; &#160; &#160;</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">Mary also likes to watch football games.</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">]</span></div></div>

<p>那么所有的单词可以构成如下形式的词袋：</p>

<div class="codeblock code-template">
    ["also", "football", "games", "john", "likes", "mary", "movies", "to", "too", "watch"]
</div>

<p>每个文本数据都可以使用这样一个<em>向量</em>的形式来表示，向量上每个维度的值为该文本中对应单词的出现频率。例如，以上第一条文本就可以表示为如下向量：</p>

<table style="width: fit-content;">
    <tr>
        <th>also</th><th>football</th><th>games</th><th>john</th><th>likes</th><th>mary</th><th>movies</th><th>to</th><th>too</th><th>watch</th>
    </tr>
    <tr>
        <td>0</td><td>0</td><td>0</td><td>1</td><td>2</td><td>1</td><td>2</td><td>1</td><td>1</td><td>1</td>
    </tr>
</table>

<p>词袋模型是自然语言处理(NLP)中最基础的一种特征提取方法，它仅考虑词频，而忽略了词语的出现位置、先后关系、词性等其它特征。尽管如此，作为一种易于使用且确实能反映文本部分特征的模型，词袋模型在文本分类、情感分析等领域仍有着广泛的应用。</p>

<p>在 Scikit-Learn 内，词袋模型由以下类提供处理方法：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">feature_extraction</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">text </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> CountVectorizer</span></div><div><span style="color: #90a4ae;">count_vect </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> CountVectorizer</span><span style="color: #39adb5;">()</span></div></div>
</div>

<p>通过 <code>.fit()</code> 方法可以将其应用于数据集中。可以使用 <code>.vocabulary_</code> 属性检查当前模型的词典及与向量维度的映射关系：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">count_vect</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">fit</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">texts</span><span style="color: #39adb5;">)</span></div><div><span style="color: #90a4ae;">count_vect</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">vocabulary_</span></div></div>    
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">{'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}</div>
</div>

<p>接下来使用一个具体的样本来演示词袋模型的特征提取结果：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">vec </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> count_vect</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">transform</span><span style="color: #39adb5;">([</span></div><div><span style="color: #90a4ae;">&#160; &#160; </span><span style="color: #39adb5;">'</span><span style="color: #91b859;">Tim likes to play games and watch baseball games.</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">])</span></div><div><span style="color: #90a4ae;">pd</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">DataFrame</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">vec</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">toarray</span><span style="color: #39adb5;">(),</span><span style="color: #90a4ae;"> </span><span style="color: #e53935;">columns</span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;">count_vect</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">get_feature_names_out</span><span style="color: #39adb5;">())</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><table class="pandas-dataframe">
        <tr>
            <th></th><th>also</th><th>football</th><th>games</th><th>john</th><th>likes</th><th>mary</th><th>movies</th><th>to</th><th>too</th><th>watch</th>
        </tr>
        <tr>
            <td>0</td><td>0</td><td>0</td><td>2</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td>
        </tr>
    </table></div>
</div>

<h3>多项式朴素贝叶斯</h3>

<p>一段文本的词频并不只有 0 和 1 两种情况，不能直接计算概率，但是可以假定词频服从多项式分布。多项式分布是二项分布的扩展，试验结果不是两种状态，而是多种互斥的离散状态。多项式分布可以描述样本出现次数的概率，因此多项式朴素贝叶斯非常适合用于特征为出现次数或者出现次数比例的情况。</p>

<p>首先使用词袋模型处理数据：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">feature_extraction</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">text </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> CountVectorizer</span></div><br><div><span style="color: #90a4ae;">count_vect </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> CountVectorizer</span><span style="color: #39adb5;">(</span><span style="color: #e53935;">stop_words</span><span style="color: #7c4dff;">=</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">english</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">)</span></div><div><span style="color: #90a4ae;">X </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> count_vect</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">fit_transform</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">mail</span><span style="color: #39adb5;">[</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">text</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">])</span></div></div>
</div>

<p>然后将其应用于多项式朴素贝叶斯模型中：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span><span style="color: #90a4ae;"> sklearn</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">naive_bayes </span><span style="color: #39adb5;font-weight: bold;">import</span><span style="color: #90a4ae;"> MultinomialNB</span></div><br><div><span style="color: #90a4ae;">model </span><span style="color: #7c4dff;">=</span><span style="color: #90a4ae;"> MultinomialNB</span><span style="color: #39adb5;">()</span></div><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">fit</span><span style="color: #39adb5;">(</span><span style="color: #90a4ae;">X</span><span style="color: #39adb5;">,</span><span style="color: #90a4ae;"> mail</span><span style="color: #39adb5;">[</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">label</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>MultinomialNB()</p></div></div>
</div>

<p>根据朴素贝叶斯模型，可以使用词频计算出不同分类下的先验概率，从而计算新的词频向量的分类概率，并将其分到概率更大的一类。以下是一个预测示例，可以看到多项式朴素贝叶斯模型成功地发现了垃圾邮件：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #90a4ae;">model</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">predict</span><span style="color: #39adb5;">(</span></div><div><span style="color: #90a4ae;">&#160; &#160; count_vect</span><span style="color: #39adb5;">.</span><span style="color: #90a4ae;">transform</span><span style="color: #39adb5;">(</span></div><div><span style="color: #90a4ae;">&#160; &#160; &#160; &#160; </span><span style="color: #39adb5;">[</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">Interested in trying out our product? </span><span style="color: #39adb5;">'</span></div><div><span style="color: #90a4ae;">&#160; &#160; &#160; &#160; &#160;</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">Here’s a special deal for newsletter subscribers: </span><span style="color: #39adb5;">'</span></div><div><span style="color: #90a4ae;">&#160; &#160; &#160; &#160; &#160;</span><span style="color: #39adb5;">'</span><span style="color: #91b859;">get 1 years of it with 3 months of extra subscription time added on top.</span><span style="color: #39adb5;">'</span><span style="color: #39adb5;">])</span></div><div><span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([1], dtype=int64)</div>
</div>

<p>总的来说，朴素贝叶斯模型是一种简单的分类算法，并且有坚实的数学原理支撑。朴素贝叶斯模型的一个显著的优点是只需要统计概率，因此计算速度很快，通常适用于维度非常高的数据集。并且由于可调参数很少，可以快速得到最终的结果，因此非常适合作为处理分类问题的基本方案，在得到数据后便可以使用朴素贝叶斯模型得到粗糙的结果。</p>
<p><a rel="nofollow" href="/archives/953">机器学习-朴素贝叶斯</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/archives/953/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>机器学习-线性回归</title>
		<link>/archives/880</link>
					<comments>/archives/880#respond</comments>
		
		<dc:creator><![CDATA[Hello]]></dc:creator>
		<pubDate>Sat, 03 Dec 2022 13:09:02 +0000</pubDate>
				<category><![CDATA[机器学习]]></category>
		<guid isPermaLink="false">/?p=880</guid>

					<description><![CDATA[<p>一元线性回归 线性回归(linear regress&#46;&#46;&#46;</p>
<p><a rel="nofollow" href="/archives/880">机器学习-线性回归</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></description>
										<content:encoded><![CDATA[
<h2>一元线性回归</h2>

<p><strong>线性回归</strong>(linear regression)应该是最简单的机器学习模型了。例如，以下是 1965~2021 年一次能源消耗量变化：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-linear-data.png" alt="" width="480">
    <figcaption>数据来源：<a href="https://www.bp.com/">bp company</a></figcaption>
</figure>

<p>容易看出自变量（年份）与因变量（一次能源消耗量）之间大致沿一条直线分布。线性回归模型就是利用线性拟合的方式，寻找自变量与因变量背后的<strong>回归直线</strong>（趋势线），再利用回归直线做简单的分析和预测。</p>

<p>线性回归根据特征变量（自变量）来预测反应变量（因变量）。由于以上的特征变量只有一个（一次能源消耗量），因此这种线性回归模型称为一元线性回归。在本文最后会考虑多元线性回归的情况。</p>

<h3>数学原理</h3>

<p>以上的样本分布规律大致成一条直线，因此可以使用直线的公式来描述它们：</p>

<div class="math">
\\[
    y = ax + b
\\]
</div>

<p>其中 <span class="math">\\( x \\)</span>为自变量，<span class="math">\\( y \\)</span> 为因变量，<span class="math">\\( a \\)</span> 为回归系数（斜率），<span class="math">\\( b \\)</span> 为截距。</p>

<p>为了发现自变量与因变量之间的关系，需要提供一系列因变量的实际值来拟合。对于每个自变量，其实际的因变量值 <span class="math">\\( y_i \\)</span> 与预测值 <span class="math">\\( \hat y_i \\)</span> 之间的接近程度可以用它们的<strong>残差平方和</strong>(Sum of Squares for Error, SSE)，即差值的平方来衡量：</p>

<div class="math">
\\[
    \sum (y_i - \hat y_i) ^ 2
\\]
</div>

<p>如果将上式展开，可以得到：</p>

<div class="math">
\\[
    \sum (y_i - (ax_i + b)) ^ 2
\\]
</div>

<p>当残差平方和最小时，说明该拟合直线最能描述实际样本的线性关系。通过对上式求导并令导数为 0 ，可以得到最小的残差平方和，即拟合最优时的回归系数 <span class="math">\\( a \\)</span> 和截距 <span class="math">\\( b \\)</span> 。</p>

<p>这种求导的过程非常简单，即便手工计算也非常方便。以上计算回归直线的方法称为<strong>最小二乘法</strong>。接下来看看如何使用代码搭建线性回归模型。</p>

<h3>代码实现：使用Scikit-Learn工具包</h3>

<p>Python 有许多提供各种机器学习算法的第三方库，<a href="http://scikit-learn.org/">Scikit-Learn</a> 是使用最广泛的一个。相比其它的机器学习库，它具有以下优点：</p>

<ul>
    <li>模型广泛，涵盖目前各种主流机器学习算法</li>
    <li>使用 NumPy 实现，在计算效率很高的同时能很好地融入 Python 科学计算的生态中</li>
    <li>完善的在线文档，帮助初学者快速入门</li>
    <li>设计得当 API 。Scikit-Learn 的所有模型都遵循几乎相同的面向对象接口，这种高度统一的 API 让使用者无需记住每一个模型是如何实现的，只需要知道算法原理和需要设置的参数即可。许多新提出的模型也采用了相同的 API 设计，从而避免了用户熟悉库操作的成本</li>
</ul>
 
<p>通过 Scikit-Learn 库可以便捷地调用一元线性回归模型。该库可以直接通过 <code>pip</code> 命令安装：</p>

<div class="codeblock code-console"><span class="token-venv">(sci)</span> PS D:\MachineLearning\demo> <span class="token-comd">pip</span> install sklearn</div>

<p>然后可以在命令行中导入并检查安装的版本：</p>

<div class="codeblock code-console">>>> import sklearn
>>> sklearn.__version__
'1.1.3' 
</div>

<p>在 Scikit-Learn 中，用于建立模型的基本数据集以二维数组表示，其中每一行表示数据集中的每个样本，而列表示构成每个样本的相关特征。因此这样的数组具有形状 <code>(n_samples, n_features)</code> ，也称为<strong>特征矩阵</strong>(features matrix)。这和 Pandas 的 <code>DataFrame</code> 是一致的，因此可以直接使用 <code>DataFrame</code> 来表示数据集。</p>

<p>特征矩阵常用变量 <code>X</code> 来表示。以上数据中自变量是一个一维数据，如果要将这些数据使用 Scikit-Learn 的线性回归模型处理，需要将每一个元素变成一个单元素数组：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>X <span style="color: #7c4dff;">=</span> x<span style="color: #39adb5;">.</span>reshape<span style="color: #39adb5;">(</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span> <span style="color: #f76d47;">1</span><span style="color: #39adb5;">)</span></div><div>X<span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([14.17022005])</div>
</div>

<p>包括线性回归在内的许多模型都需要通过研究自变量和因变量的关系，来预测新加入样本的值。因此除了特征矩阵 <code>X</code> 之外，还需要为每个样本准备一个对应的结果（目标数组），通常简记为 <code>y</code>。目标数组一般是一维数组，其长度就是样本总数 <code>n_samples</code> ，因此可以使用一维的 NumPy 数组或 Pandas 的 <code>Series</code> 表示。</p>

<p>有了数据以后，就可以用其建立模型了。在 Scikit-Learn 中，每个模型都是一个 Python 类。在建立模型前，需要从对应的模块中导入需要的类：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>linear_model <span style="color: #39adb5;font-weight: bold;">import</span> LinearRegressor</div></div>
</div>

<p>建立一个具体的模型就是实例化类。有一些重要的模型参数必须在应用于数据集前便确定好，这些参数通常被称为超参数，通常在初始化方法中作为参数传入。</p>

<p>一个模型的超参数可能有几十个，好在 Scikit-Learn 为大多不常用的超参数都提供了合适的默认值。只要调整少量的主要超参数即可。例如，假设线性回归模型需要一并拟合直线的截距，可以在初始化时设置超参数 <code>fit_intercept</code> 为 <code>True</code> ：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model <span style="color: #7c4dff;">=</span> LinearRegression<span style="color: #39adb5;">(</span><span style="color: #e53935;">fit_intercept</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">True</span><span style="color: #39adb5;">)</span></div></div>
</div>

<p>在初始化模型以后，需要将数据集应用到其中。这一步可以通过模型的 <code>.fit()</code> 方法实现：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">,</span> y<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>LinearRegression()</p></div></div>
</div>

<p>该方法会根据提供的特征矩阵和目标数组完成必要的计算，计算完成后可以得到一些模型参数。这些模型参数一般都以单下划线结尾的实例属性表示，例如线性回归模型常用的结果有回归直线的斜率和截距：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model<span style="color: #39adb5;">.</span>coef_</div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([7.57467915])</div>

    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model<span style="color: #39adb5;">.</span>intercept_</div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">-14726.826081799325</div>
</div>

<p>利用搭建好的模型，可以使用 <code>.predict()</code> 方法根据一个新的自变量预测因变量的值：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model<span style="color: #39adb5;">.</span>predict<span style="color: #39adb5;">([[</span><span style="color: #f76d47;">2030</span><span style="color: #39adb5;">],</span> <span style="color: #39adb5;">[</span><span style="color: #f76d47;">2050</span><span style="color: #39adb5;">],</span> <span style="color: #39adb5;">[</span><span style="color: #f76d47;">2100</span><span style="color: #39adb5;">]])</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([ 649.77260241,  801.26618551, 1180.00014325])</div>
</div>

<p>这一步等价于将新的 <span class="math">\\( x_i \\)</span> 代入方程 <span class="math">\\( y=ax+b \\)</span> 中求解。注意自变量同样需要写成二维数组的模式。从预测结果中可以看到 10 年、50 年和 100 年后的一次能源消耗量。</p>

<p>如果在一个区间内尝试预测足够多的点，并将预测结果绘制出来，就可以看到一条趋势线了：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>X_fit <span style="color: #7c4dff;">=</span> np<span style="color: #39adb5;">.</span>linspace<span style="color: #39adb5;">(</span><span style="color: #f76d47;">10</span><span style="color: #39adb5;">,</span> <span style="color: #f76d47;">20</span><span style="color: #39adb5;">,</span> <span style="color: #f76d47;">1000</span><span style="color: #39adb5;">).</span>reshape<span style="color: #39adb5;">((</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span> <span style="color: #f76d47;">1</span><span style="color: #39adb5;">))</span></div><div>y_fit <span style="color: #7c4dff;">=</span> model<span style="color: #39adb5;">.</span>predict<span style="color: #39adb5;">(</span>X_fit<span style="color: #39adb5;">)</span></div><div>plt<span style="color: #39adb5;">.</span>plot<span style="color: #39adb5;">(</span>X_fit<span style="color: #39adb5;">,</span> y_fit<span style="color: #39adb5;">)</span></div><div>plt<span style="color: #39adb5;">.</span>scatter<span style="color: #39adb5;">(</span>x<span style="color: #39adb5;">,</span> y<span style="color: #39adb5;">,</span> <span style="color: #e53935;">s</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">30</span><span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><img decoding="async" class="matplotlib-output" src="/wordpress/wp-content/uploads/2022/12/01-output-linear-predict.png" alt=""></div>
</div>

<p>可以看到模型中 <code>.predict()</code> 依据的原理就是从趋势线上选择对应的点。</p>

<h3>结果分析</h3>

<p>以上建立了线性回归的模型并对新的数据作出预测。但是还需要对模型的结果做一些分析。</p>

<p>首要的问题是数据能否应用与线性回归模型。以上的数据较为清晰地呈现出一条直线，但很多时候却未必。例如，以下是一个对看似为线性的数据应用线性回归模型的结果：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-linear-uncertain.png" alt="" width="460">
    <figcaption>一个可能呈现为直线的数据</figcaption>
</figure>

<p>线性回归的模型评估主要以3个值进行作为评价标准：R-square（即 <span class="math">\\( R^2 \\)</span> ）、Adj. R-square（即 <span class="math">\\( \text{Adjusted}\, R^2 \\)</span>） 和 <span class="math">\\( P \\)</span> 值。对于一元线性回归，主要关心的是它的 <span class="math">\\( R^2 \\)</span> 值。</p>

<p>设 <span class="math">\\( Y_\text{i} \\)</span> 为实际值，<span class="math">\\( Y_{\text{fitted}} \\)</span> 为预测值，<span class="math">\\( Y_{\text{mean}} \\)</span> 为所有散点的平均值，则数据的整体平方和 <span class="math">\\( \text{TSS} \\)</span> 、残差平方和 <span class="math">\\( \text{RSS} \\)</span> 以及解释平方和 <span class="math">\\( \text{ESS} \\)</span> 可以根据以下公式计算：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-linear-R-square.png" alt="" width="500">
</figure>

<p>对 <span class="math">\\( R^2 \\)</span> ，有如下计算公式：</p>

<div class="math">
\\[
    R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}
\\]
</div>

<p>对于线性回归模型，如果拟合度较高，则残差平方和 <span class="math">\\( \text{RSS} \\)</span> 需要尽可能小，即 <span class="math">\\( R^2 \\)</span> 尽可能大。为了避免数据的数量级对结果的影响，需要将其与整体平方和 <span class="math">\\( \text{TSS} \\)</span> 对比，当 <span class="math">\\( R^2 \\)</span> 趋向 1 时，说明 <span class="math">\\( \text{RSS} \\)</span> 远小于 <span class="math">\\( \text{TSS} \\)</span> ，即基本所有点落在回归直线上。</p>

<p>Scikit-Learn 自带的函数就可以计算 R-square 值，如下：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn.metrics <span style="color: #39adb5;font-weight: bold;">import</span> r2_score</div><div>r2_score<span style="color: #39adb5;">(</span>Y, model.predict<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">))</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.986</div>
</div>

<p><code>metrics</code> 是 Scikit-Learn 包含的一个模块，里面有各种计算用的函数。为了后续分析其它指标的方便，接下来再介绍一个用于分析模型的工具。</p>

<p><a href="https://www.statsmodels.org/stable/index.html">statsmodels</a> 是一个探究数据统计模型的 Python 第三方库。可以通过 <code>pip</code> 直接安装它：</p>

<div class="codeblock code-console"><span class="token-venv">(sci)</span> PS D:\MachineLearning\demo> <span class="token-comd">pip</span> install statsmodels</div>

<p>然后可以通过以下代码分析搭建回归模型的各种数据信息：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">import</span> statsmodels<span style="color: #39adb5;">.</span>api <span style="color: #39adb5;font-weight: bold;">as</span> sm</div><div>est <span style="color: #7c4dff;">=</span> sm<span style="color: #39adb5;">.</span>OLS<span style="color: #39adb5;">(</span>y<span style="color: #39adb5;">,</span> sm<span style="color: #39adb5;">.</span>add_constant<span style="color: #39adb5;">(</span>x<span style="color: #39adb5;">)).</span>fit<span style="color: #39adb5;">()</span></div><div>est<span style="color: #39adb5;">.</span>summary<span style="color: #39adb5;">()</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><table class="pandas-dataframe">
        <caption>OLS Regression Results</caption>
        <tr>
            <th>Dep. Variable:</th><td>consumption</td><th class="emphasis">R-squared:</th><td class="emphasis">0.986</td>
        </tr>
        <tr>
            <th>Model:</th><td>OLS</td><th class="emphasis">Adj. R-squared:</th><td class="emphasis">0.986</td>
        </tr>
        <tr>
            <th>Method:</th><td>Least Squares</td><th>F-statistic:</th><td>3962.</td>
        </tr>
        <tr>
            <th>Date:</th><td>Sat, 26 Nov 2022</td><th>Prob (F-statistic):</th><td>6.12e-53</td>
        </tr>
    </table><table class="pandas-dataframe">
        <tr>
            <th></th><th class="emphasis">coef</th><th>std err</th><th>t</th><th class="emphasis">P>|t|</th><th>[0.025</th><th>0.975]</th>
        </tr>
        <tr>
            <td>const</td><td class="emphasis">-1.473e+04</td><td>239.859</td><td>-61.398</td><td class="emphasis">0.000</td><td>-1.52e+04</td><td>-1.42e+04</td>
        </tr>
        <tr>
            <td>year</td><td class="emphasis">7.5747</td><td>0.120</td><td>62.941</td><td class="emphasis">0.000</td><td>7.333</td><td>7.816</td>
        </tr>
    </table><table class="pandas-dataframe">
        <tr>
            <th>Skew:</th><td>-0.436</td><th>Prob(JB):</th><td>0.151</td>
        </tr>
        <tr>
            <th>Kurtosis:</th><td>2.087</td><th>Cond. No.</th><td>2.41e+05</td>
        </tr>
    </table><span class="notes">Notes:</span>
<span class="notes">[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</span>
<span class="notes">[2] The condition number is large, 2.41e+05. This might indicate that there are strong multicollinearity or other numerical problems.</span></div>
</div>

<p><code>OLS</code> 表示普通最小二乘法(Ordinary Least Squares)，用于分析线性回归模型的各种统计值。<code>sm.add_constant()</code> 函数是为了给原来的特征变量 <code>X</code> 添加常数项，即公式 <span class="math">\\( y=ax+b \\)</span> 中的截距 <span class="math">\\( b \\)</span> 。</p>

<p>以上结果省略了部分不重要的数据，主要的结果已经用红色强调出了。在左上角就是模型的 R-square 值。在不同应用场合下 R-square 值的标准也不一样，一般在 0.7 以上就可以认为具有较好的线性相关性，0.9 线性相关性已经很强了。本数据的 R-square 达到了 0.98 ，完全可以认为一次能源消耗量和年份有着很强的线性关系。</p>

<h2>线性回归模型的拓展</h2>

<h3>多元线性回归</h3>

<p>除了一个自变量的直线拟合，还可以处理多个自变量的线性回归模型。<strong>多元线性回归</strong>是推广到多个特征变量的线性回归模型，可以考虑多个因素对目标结果的影响。</p>

<p>多元线性回归可以表示为如下所示的公式：</p>

<div class="math">
\\[
    y = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n + \dots
\\]
</div>

<p>其中 <span class="math">\\( x_i \\)</span> 为不同的特征变量，<span class="math">\\( \theta_i \\)</span> 为这些特征变量前的系数（影响因素），<span class="math">\\( \theta_0 \\)</span> 为常数项。这个数学公式表示的是三维空间中的一个平面，或者是更高维度的一个超平面。</p>

<p>多元线性回归也可以表示为如下的矩阵形式：</p>

<div class="math">
    \\[
        y = \boldsymbol{\theta} \cdot \boldsymbol{X} 
    \\]
</div>
    
<p>这里 <span class="math">\\( \boldsymbol{\theta} \\)</span> 是模型的参数矩阵，<span class="math">\\( \boldsymbol{X} \\)</span> 是特征变量向量。因此多元线性回归的模型和一元线性回归是类似的，只是训练时用的 <code>X</code> 每个数组内的元素不止一个而已。</p>

<p>多元线性回归模型的的原理也是取合适系数，使残差平方和</p>

<div class="math">
\\[
    \sum (y_i - \hat y_i)^2
\\]
</div>

<p>最小。</p>

<p>数学上主要通过最小二乘法和梯度下降法来计算合适的系数。</p>

<p>接下来通过一个具体的示例介绍多元线性回归的分析。假设某公司不同时期的利润（因变量）与各成本（自变量）之间的关系为：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-multilinear-factors.png" width="460" alt="">
    <figcaption>（数据是瞎编的）</figcaption>
</figure>

<p>在建立模型前，首先需要分析数据是否可以建立多元线性回归模型。对于多元线性回归，衡量结果好坏的评估值还有两个：Adj. R-square 和 <span class="math">\\( P \\)</span> 值。</p>

<p>Adj. R-square 是改进的 R-square ，目的是防止选取特征变量过多导致 R-square 值虚高。Adj. R-square 在 R-square 的基础上考虑了特征变量的数量这一因素，其公式为：</p>

<div class="math">
\\[
    \bar{R^2} = 1 - \frac{(1 - R^2)(n - 1)}{n - k - 1}
\\]
</div>

<p>其中 <span class="math">\\( n \\)</span> 为样本数量，<span class="math">\\( k \\)</span> 为特征变量数量。因此选择的特征变量越多，对 Adj. R-square 影响便越大。</p>

<p><span class="math">\\( P \\)</span> 值是统计学假设检验的一个概念，表示假定特征变量与目标变量有显著相关性时，所得到的样本出现不符合假设的结果概率：若概率越大，则 <span class="math">\\( P \\)</span> 值越大，说明特征变量与目标变量有显著相关性的可能性便越小；若 <span class="math">\\( P \\)</span> 值越小，说明更可能有显著相关性。统计学中通常以 <span class="math">\\( P \\)</span> 值 0.05 为特征变量与目标变量是否有显著相关性的阈值。</p>

<p>总体来说，线性拟合常用的几个评价标准为：</p>

<ul>
    <li>R-square 和 Adj. R-square 衡量线性拟合的优劣</li>
    <li><span class="math">\\( P \\)</span> 值衡量特征变量的显著性</li>
</ul>

<p>接下来使用 statsmodels 来检验多元线性回归的数据：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>sm<span style="color: #39adb5;">.</span>OLS<span style="color: #39adb5;">(</span>y_mul<span style="color: #39adb5;">,</span> sm<span style="color: #39adb5;">.</span>add_constant<span style="color: #39adb5;">(</span>X_mul<span style="color: #39adb5;">)).</span>fit<span style="color: #39adb5;">().</span>summary<span style="color: #39adb5;">()</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><table class="pandas-dataframe">
        <caption>OLS Regression Results</caption>
        <tr>
            <th>Dep. Variable:</th><td>Profit</td><th class="emphasis">R-squared:</th><td class="emphasis">0.952</td>
        </tr>
        <tr>
            <th>Model:</th><td>OLS</td><th class="emphasis">Adj. R-squared:</th><td class="emphasis">0.948</td>
        </tr>
        <tr>
            <th>Method:</th><td>Least Squares</td><th>F-statistic:</th><td>222.3</td>
        </tr>
    </table><table class="pandas-dataframe">
        <tr>
            <th></th><th>coef</th><th>std err</th><th>t</th><th class="emphasis">P>|t|</th><th>[0.025</th><th>0.975]</th>
        </tr>
        <tr>
            <td>const</td><td>5.009e+04</td><td>6571.034</td><td>7.622</td><td class="emphasis">0.000</td><td>3.69e+04</td><td>6.33e+04</td>
        </tr>
        <tr>
            <td>R&D Spend</td><td>0.7762</td><td>0.054</td><td>14.442</td><td class="emphasis">0.000</td><td>0.668</td><td>0.885</td>
        </tr>
        <tr>
            <td>Administration</td><td>-0.0280</td><td>0.051</td><td>-0.548</td><td class="emphasis">0.586</td><td>-0.131</td><td>0.075</td>
        </tr>
        <tr>
            <td>Marketing Spend</td><td>0.0270</td><td>0.016</td><td>1.639</td><td class="emphasis">0.108</td><td>-0.006</td><td>0.060</td>
        </tr>
        <tr>
            <td>Production</td><td>1.2157</td><td>1.204</td><td>1.010</td><td class="emphasis">0.318</td><td>-1.209</td><td>3.641</td>
        </tr>
    </table></div>
</div>

<p>在以上结果中，R-square 和 Adj. R-square 都达到了 0.9 以上，说明模型的线性拟合程度很好。在几个特征变量内，只有 R&D Spend 的 <span class="math">\\( P \\)</span> 值达到了阈值 0.05 以内，说明利润与研发费用具有很强的线性相关性；产量和管理费用的 <span class="math">\\( P \\)</span> 值都很大，说明它们与利润不太可能有线性相关性，在建立回归模型时可以忽略它们。</p>

<!-- 
有一个可以直接得出结果的数学方程：

<div class="math">
\\[
    \boldsymbol{\hat K} = (\boldsymbol X ^T \boldsymbol X)^{-1} \boldsymbol X ^T y
\\]
</div>

不过这种方法时间复杂度有点高：<span class="math">\\( O(N^{2.4}) \\)</span> 到 <span class="math">\\( O(N^3) \\)</span> 。
-->

<h3>多项式回归</h3>

<p>很多时候数据在散点图中未必表现为一条直线，那么就不能使用直线的方程去描述它，否则会产生<strong>欠拟合</strong>：模型没有捕捉到数据的特征，不能用于拟合数据，如图所示：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-polynomial-bad.png" width="440" alt="">
    <figcaption>曲线数据不能用直线拟合</figcaption>
</figure>

<p>对于曲线数据，可以通过基函数对原始数据添加新的特征，使其由 <span class="math">\\( [ \, x \, ] \\)</span> 变换为形如 <span class="math">\\( [ \, 1, \, x, \, x^2, \dots, x^n \, ] \\)</span> 的多项式组合形式，从而将变量间的线性回归模型转换为非线性回归模型。其中第一列 1 为常数项，它的值对分析结果没有影响。</p>

<p><code>preprocessing</code> 是 Scikit-Learn 的一个模块，提供数据预处理所需要的工具，其中就包含多项式变换的功能：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>preprocessing <span style="color: #39adb5;font-weight: bold;">import</span> PolynomialFeatures</div><div>poly <span style="color: #7c4dff;">=</span> PolynomialFeatures<span style="color: #39adb5;">(</span><span style="color: #e53935;">degree</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">2</span><span style="color: #39adb5;">)</span></div><div>X_poly <span style="color: #7c4dff;">=</span> poly<span style="color: #39adb5;">.</span>fit_transform<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">)</span></div><div>X_poly<span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">array([ 1.        , -0.36537882,  0.13350168])</div>
</div>

<p>可以将变换得到的数据应用到线性回归模型中：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>good_model <span style="color: #7c4dff;">=</span> LinearRegression<span style="color: #39adb5;">()</span></div><div>good_model<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X_poly<span style="color: #39adb5;">,</span> y<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>LinearRegression()</p></div></div>
</div>

<p>这样得到的回归趋势线就是一条拟合程度较好的曲线了：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-polynomial-good.png" width="440" alt="">
</figure>

<p>除此之外，还可以将预处理和建立模型结合起来，使用 Scikit-Learn 提供的管道操作组合成一个模型，这样便无需频繁处理数据了：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>pipeline <span style="color: #39adb5;font-weight: bold;">import</span> make_pipeline</div><br><div>model <span style="color: #7c4dff;">=</span> make_pipeline<span style="color: #39adb5;">(</span>PolynomialFeatures<span style="color: #39adb5;">(</span><span style="color: #f76d47;">3</span><span style="color: #39adb5;">),</span> </div><div>&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; LinearRegression<span style="color: #39adb5;">())</span></div><div>model<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>x<span style="color: #39adb5;">.</span>reshape<span style="color: #39adb5;">(</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span> <span style="color: #f76d47;">1</span><span style="color: #39adb5;">),</span> y<span style="color: #39adb5;">)</span></div></div>
</div>

<h3>正则化线性模型</h3>

<p>多项式回归等曲线回归模型都存在一个问题，那就是曲线的形状不易确定。例如，多项式回归的次数往往不易确定，如果设置了过高的多项式次数，可能会导致很糟糕的拟合结果。</p>

<p>例如，以下是对一组曲线数据分别应用 3 次和 16 次多项式的拟合结果：</p>

<figure class="parallel">
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-polynomial-uncertain.png" width="370" alt="">
        <figcaption>3 次多项式的拟合结果</figcaption>
    </div>
    <div>
        <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-polynomial-worse.png" width="370" alt="">
        <figcaption>16 次多项式的拟合结果</figcaption>
    </div>
</figure>

<p>尽管 16 次多项式拟合后曲线更贴近原始数据，但并不能说明这就是一个合适的趋势线：不能为了盲目增加 <span class="math">\\( R^2 \\)</span> 而增加回归曲线的次数，否则可能发生<strong>过拟合</strong>，这是一个与欠拟合相对应的概念，得出的数据不具有推广性。特别是在横坐标接近 0 的位置曲线有很大幅度的变动，这显然是不合理的。</p>

<!-- 
<div class="math">
\\[
    \text{MSE} = \frac 1 n \sum_{i=1}^{n} (\boldsymbol{\theta}^T\boldsymbol{x}_i-y_i)^2
\\]
</div> 
-->

<p>由于残差平方无法判断是否会发生过拟合，减少过拟合的一个方法是对该公式添加额外的约束项，即正则化(regularization)：更多的约束可以防止模型的某些参数不正确地过分增大而发生过拟合。</p>

<p>正则化的常用方法有两个：岭回归和 Lasso 回归。</p>

<p>岭回归(ridge regression)也称为 Tikhonov 正则化，它在描述模型最优方案的公式（残差平方和）额外添加了一个约束项：</p>

<div class="math">
\\[
    \alpha \sum_{i=1}^{n} \theta_i^2
\\]
</div>

<p>参数 <span class="math">\\( \alpha \\)</span> 用于控制约束效果：趋向于 0 的参数约束的力度更小。从公式上看，使用岭回归可以避免线性模型为了盲目贴近数据而采用很大的多项式系数 <span class="math">\\( \theta_i \\)</span> 而导致曲线过于陡峭。</p>

<p>岭回归也和线性回归放在同样的模块内，并遵循类似的接口规则：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>linear_model <span style="color: #39adb5;font-weight: bold;">import</span> Ridge</div><div>model <span style="color: #7c4dff;">=</span> Ridge<span style="color: #39adb5;">(</span><span style="color: #e53935;">alpha</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">0.2</span><span style="color: #39adb5;">)</span></div><div>model<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X_poly<span style="color: #39adb5;">,</span> y<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><div class="sklearn-model"><p>Ridge(alpha=0.2)</p></div></div>
</div>

<p>下图展示了不同参数下岭回归的结果：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-ridge-results.png" width="440" alt="">
</figure>

<p>另一种常用的正则化是 Lasso 正则化，它的约束项为：</p>

<div class="math">
    \\[
        \alpha \sum_{i=1}^{n} |\theta_i|
    \\]
</div>

<p>从公式上看，Lasso 回归与岭回归差别不大，但 Lasso 回归在数学上更倾向于将特征系数设置为 0 。假设有两个特征变量，那么 Lasso 回归在降低正则化项的和时，更容易接近坐标轴而使某个系数趋近 0 ，如下图所示：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-lasso-losses-zero.png" width="780" alt="">
</figure>

<p>可以从同样的地方导入 <code>Lasso</code> 类建立模型。下图展示了不同参数下 Lasso 回归的结果，可以看到都比较趋近于 3 次多项式：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/12/01-output-lasso-results.png" width="440" alt="">
</figure>

<p>总的来说，线性回归作为统计学中常用的一个工具，它的数学原理已经被研究得很透彻了，因此模型的可解释性很强，且计算速度快，对于线性数据的预测结果很好。但是线性回归不适用于非线性数据，且无法处理分类问题，因此需要提前判断数据是否有线性关系。</p>
<p><a rel="nofollow" href="/archives/880">机器学习-线性回归</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/archives/880/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>机器学习-数据聚类与分群</title>
		<link>/archives/851</link>
					<comments>/archives/851#respond</comments>
		
		<dc:creator><![CDATA[Hello]]></dc:creator>
		<pubDate>Mon, 21 Nov 2022 08:09:06 +0000</pubDate>
				<category><![CDATA[机器学习]]></category>
		<guid isPermaLink="false">/?p=851</guid>

					<description><![CDATA[<p>聚类就是将样本划分为由具有类似的特征组成的多个类。在&#46;&#46;&#46;</p>
<p><a rel="nofollow" href="/archives/851">机器学习-数据聚类与分群</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></description>
										<content:encoded><![CDATA[
<p>聚类就是将样本划分为由具有类似的特征组成的多个类。在实际分析一堆数据时，往往会只给出这些数据的特征变量，但不知道这些数据应该如何分类，这就需要对已有数据根据性质分组。</p>

<p>聚类有两种常用的算法：KMeans 和 DBSCAN 。</p>

<h2>KMeans算法</h2>

<h3>算法原理</h3>

<p><strong>KMeans 算法</strong>是最简单、最容易理解的聚类算法，其中 <em>K</em> 代表数据，<em>Means</em> 代表每个类别内样本的均值。顾名思义，它是用数据的均值对其分类。</p>

<p>例如，对于如下散点的分布，可以比较清晰地看出来它大致聚为四个集群：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-points.png" alt="" width="360">
</figure>

<p>之所以可以看出它们形成了四个集群，因为每个集群都围绕着一个<em>中心点</em>分布，如果点离这个中心的<em>距离</em>更近，那么就容易看出该点属于这个集群。</p>

<p>这就是 KMeans 算法的原理，KMeans 算法以距离作为样本间相似度的度量标准，将距离相近的样本分配至同一个类别。通常可以采用两点间的直线距离（欧几里得距离）来度量各样本间的距离。</p>

<p>KMeans 算法最重要的就是计算中心点。直接计算中心点的话计算量很大，而且处理较为复杂。因此，KMeans 使用一种称为<strong>期望最大化</strong>(expectation-maximization, E-M)的算法来处理这个问题。期望最大化算法应用于数据科学的很多场景中，KMeans 是该算法的一个非常简单并且易于理解的应用。</p>

<p>期望最大化算法包含以下步骤：</p>

<ol>
    <li>期望步骤(E-step)：将样本点分配给最近的集群中心点代表的类别，更新每个点是属于哪一个集群的<em>期望</em>值</li>
    <li>最大化步骤(M-step)：将集群中心点设置为该集群所有点坐标的平均值，得到集群关于中心点拟合函数<em>最大化</em>时对应的坐标</li>
</ol>

<p>最开始的中心点可以通过在现有点中随机选取得到。每一次重复期望最大化步骤都将会得到更好的聚类效果，直到前后两次分类结果没有差别为止。</p>

<p>下图展示了 KMeans 聚类时的可视化迭代过程：</p>

<figure class="mid">
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-Iterations.png" alt="" width="650">
</figure>

<p>也可以借助可视化网站 <a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering/">https://www.naftaliharris.com/blog/visualizing-k-means-clustering/</a> 交互式查看 KMeans 算法聚类的过程。该网站还给出了许多其它算法的交互式演示，包括接下来介绍的 DBSCAN 聚类算法。</p>

<h3>代码实现</h3>

<p>以上介绍了 KMeans 的原理。在实际应用中，可以直接使用 Scikit-Learn 提供的工具完成 KMeans 聚类，它与 Scikit-Learn 其它工具的接口是一致的。KMeans 模型的主要参数 <code>n_clusters</code> 即 K 值，表示样本聚成的类数：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>cluster <span style="color: #39adb5;font-weight: bold;">import</span> KMeans</div><div>kms <span style="color: #7c4dff;">=</span> KMeans<span style="color: #39adb5;">(</span><span style="color: #e53935;">n_clusters</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">7</span><span style="color: #39adb5;">)</span></div><div>kms<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>points<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">KMeans(n_clusters=7)</div>
</div>

<p>当聚类完成后，可以使用 <code>.labels_</code> 属性获取聚类结果，<code>.cluster_centers_</code> 属性获取每一个类别的中心点。</p>

<p>以下将所有样本点和聚类的中心点以散点图的形式表现出来，并根据聚合成的不同类别以不同颜色区分结果：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>plt<span style="color: #39adb5;">.</span>scatter<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">[:,</span> <span style="color: #f76d47;">0</span><span style="color: #39adb5;">],</span> X<span style="color: #39adb5;">[:,</span> <span style="color: #f76d47;">1</span><span style="color: #39adb5;">],</span> <span style="color: #e53935;">c</span><span style="color: #7c4dff;">=</span>cluster<span style="color: #39adb5;">.</span>labels_<span style="color: #39adb5;">,</span> <span style="color: #e53935;">cmap</span><span style="color: #7c4dff;">=</span><span token="string">'Wistia'</span><span style="color: #39adb5;">)</span></div><div>centers <span style="color: #7c4dff;">=</span> cluster<span style="color: #39adb5;">.</span>cluster_centers_</div><div>plt<span style="color: #39adb5;">.</span>scatter<span style="color: #39adb5;">(</span>centers<span style="color: #39adb5;">[:,</span> <span style="color: #f76d47;">0</span><span style="color: #39adb5;">],</span> centers<span style="color: #39adb5;">[:,</span> <span style="color: #f76d47;">1</span><span style="color: #39adb5;">],</span> <span style="color: #e53935;">c</span><span style="color: #7c4dff;">=</span><span token="string">'b'</span><span style="color: #39adb5;">,</span> <span style="color: #e53935;">s</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">200</span><span style="color: #39adb5;">,</span> <span style="color: #e53935;">alpha</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">0.5</span><span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output"><img decoding="async" class="matplotlib-output" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-result.png" alt=""></div>
</div>

<p>结果整体来说划分良好。</p>

<h3>结果分析</h3>

<p>由于 KMeans 算法是一种期望最大化算法，它会在每一步中改进结果以向着更优解的方向移动，但是这并不保证可以到达全局最优解。初始中心点的选择会在较大程度上影响聚类结果，某些随机值带来的初始中心点可能会导致仅收敛到局部最优解，但整体聚类结果并不理想：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-bad-situation.png" alt="" width="540">
    <figcaption>不好的随机种子带来的不好收敛结果</figcaption>
</figure>

<p>基于此，Scikit-Learn 的 KMeans 模型提供了 <code>n_init</code> 参数，用于多次使用不同的随机种子运行模型，并选取最好的结果。该参数的默认值是 10 ，即从 10 次不同的测试中选择最好结果。</p>

<p>多次运行还需要分析哪个是最好结果，评价 KMeans 模型的一项常用的指标是各个点到聚类中心距离的平方和，称为惯性(inertia)或 WSS(Within-Cluster-Sum of Squared)。在聚合成相同数量的集群时，WSS 越小，则说明模型越好。在建立模型后，可以使用 <code>.inertia_</code> 属性检查当前模型的 WSS 值。</p>

<p>下图对比了不同聚类结果下的 WSS 值：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-seed-WSS.png" alt="" width="540">
</figure>

<p>在之前的模型建立中，通过绘制散点图来观察数据大致可以被聚合成的类别数量。但在没有提前指定数据类别数量的情况下，如果数据维度很高或者数据量过大，那么就无法以直观的形式检查它可能的类别数量。</p>

<p>有些时候即便以可视化的形式展现出来，也未必能很好地判断聚合类别数。例如，考虑以下数据点集合在不同聚类数下的结果：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-diff-k.png" alt="" width="540">
</figure>

<p>检查结果可以发现，当聚合成 2 、3 、4 类时的结果似乎都有一定道理。</p>

<p>可以使用 WSS 来判断最好的结果，但是随着聚类个数的增加，聚类中心也变多了，各个点到聚类中心的距离肯定更近。如果将不同聚类数以及对应的最好的 WSS 值绘制在图表上，会得到以下折线：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-K-WSS.jpg" alt="" width="480">
</figure>

<p>在折线上通常会出现一个拐点，这个拐点称为“肘”(elbow)。在肘部对应的 K 值之前，每增加一个分类数可以使 WSS 快速下降；但在肘部之后，更多的分类数对 WSS 的影响并不明显。在不清楚合适分类数的情况下，肘部对应的 K 值是一个不错的选择。</p>

<p>如果将折线的两端连成一条直线，那么肘部在 <span class="math">\\( y \\)</span> 方向上离这条直线的距离一般最远。因此，可以使用以下代码计算肘部对应的 K 值：</p>

<div class="vscode-block"><div>slope <span style="color: #7c4dff;">=</span> <span style="color: #39adb5;">(</span>WSS<span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span> <span style="color: #7c4dff;">-</span> WSS<span style="color: #39adb5;">[</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">])</span> <span style="color: #7c4dff;">/</span> <span style="color: #39adb5;">(</span>Krange<span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span> <span style="color: #7c4dff;">-</span> Krange<span style="color: #39adb5;">[</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">])</span></div><div>intercept <span style="color: #7c4dff;">=</span> WSS<span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span> <span style="color: #7c4dff;">-</span> slope <span style="color: #7c4dff;">*</span> Krange<span style="color: #39adb5;">[</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">]</span></div><div>y <span style="color: #7c4dff;">=</span> Krange <span style="color: #7c4dff;">*</span> slope <span style="color: #7c4dff;">+</span> intercept</div><div>best_k <span style="color: #7c4dff;">=</span> Krange<span style="color: #39adb5;">[(</span>y <span style="color: #7c4dff;">-</span> WSS<span style="color: #39adb5;">).</span>argmax<span style="color: #39adb5;">()]</span></div></div>

<p>这种方式比较简单，但结果比较粗糙。一种更精确但也更复杂的方法是使用<strong>轮廓系数</strong>(silhouette value)。样本点轮廓系数的计算公式为：</p>

<div class="math">
\\[
    s(i) = \frac{b(i) - a(i)}{\max \{ a(i), b(i) \}}
\\]
</div>

<p>其中 <span class="math">\\( a(i) \\)</span> 是样本 <span class="math">\\( i \\)</span> 与同一集群中其它样本的平均距离，<span class="math">\\( b(i) \\)</span> 是平均最近集群距离（即到不包括自身集群的最近集群实例的平均距离）。轮廓系数的取值范围是 <span class="math">\\( [-1, +1] \\)</span> ，该值越接近 +1 ，表示对应的样本点越好地位于自身的集群中，并且远离其它集群；接近 0 的轮廓系数表示它接近一个集群的边界；接近 -1 的系数意味着样本点可能分配给了错误的集群。</p>

<p>可以使用以下代码计算轮廓分数（所有实例的平均轮廓系数）：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>metrics <span style="color: #39adb5;font-weight: bold;">import</span> silhouette_score</div><div>silhouette_score<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">,</span> cluster<span style="color: #39adb5;">.</span>labels_<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">0.320434040824261</div>
</div>

<p>将不同聚类数的轮廓分数以折线图的形式绘制出来，结果为：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-silhouette-scores.png" alt="" width="480">
</figure>

<p>从结果中可以看到，虽然使用肘点判断结果为 <span class="math">\\( k=3 \\)</span> 时效果最好，但轮廓系数判断该值使得一个集群可能会被不合理地拆分，使得许多样本点落在了它们的分界线上，说明 <span class="math">\\( k=2 \\)</span> 可能是一个更好的集群数。</p>

<p>Scikit-Learn 还提供了 KMeans 算法的一个变种，它在每次迭代中采用随机抽样的方法选择小批量的数据子集，在减小计算时间的同时能够尽可能的拟合原始的数据。这在数据集过大时可以显著提升算法的速度，并且可以处理内存无法容纳的超大数据集。可以使用 <code>MiniBatchKMeans</code> 类调用该算法，使用方式和 <code>KMeans</code> 类是一致的：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>cluster <span style="color: #39adb5;font-weight: bold;">import</span> MiniBatchKMeans</div></div>
    <div class="jupyter-separator"></div>
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #7c4dff;">%%</span>timeit</div><div>cluster <span style="color: #7c4dff;">=</span> KMeans<span style="color: #39adb5;">(</span><span style="color: #e53935;">n_clusters</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">12</span><span style="color: #39adb5;">)</span></div><div>cluster<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">744 ms ± 81.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</div>
    <div class="jupyter-separator"></div>
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #7c4dff;">%%</span>timeit</div><div>cluster <span style="color: #7c4dff;">=</span> MiniBatchKMeans<span style="color: #39adb5;">(</span><span style="color: #e53935;">n_clusters</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">12</span><span style="color: #39adb5;">)</span></div><div>cluster<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">392 ms ± 74.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</div>
</div>

<p>KMeans 是一种经典的聚类算法，它适用于任意维度数据的聚类，但在聚类时需要提前知道或判断 K 值。KMeans 的聚类依据是更接近的集群中心点，这使得在实际聚类时每个集群边界总是线性的。使用 Voronoi 图可以很清晰地表现这一原理，图中落在由中心点确定的多边形内的样本会被归为该类：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-KMeans-Voronoi-diagram.jpg" alt="" width="480">
</figure>

<p>由于只能确定线性聚类边界，当集群呈现非线性的复杂形状时，KMeans 往往会不起作用，例如以下数据：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-KMeans-Cons.png" alt="" width="480">
    <figcaption>KMeans 算法无法处理非线性边界</figcaption>
</figure>

<p>这时候就不能使用 KMeans 了。不过有一种算法可以很好地处理这种形式的聚类问题：DBSCAN。</p>

<h2>DBSCAN算法</h2>

<h3>算法原理</h3>

<p>DBSCAN 算法全称 Density-Based Spatial Clustering of Applications with Noise ，是一种以密度为基础的空间聚类算法，可以用密度的形式发现任意形状的集群。该算法将集群定义为具有足够密度的连续区域，因此可以处理任意形状的边界，如下图所示：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-DBSCAN-density.jpg" alt="" width="480">
</figure>

<p>DBSCAN 算法的基本步骤为：先随机选取一个点，以该样本点为圆心，按照设定的半径作圆。如果圆内的样本数大于等于设定的阈值（密度），则将这些样本归为一类。接着选定圆内的其它样本点，继续作圆；如果某个圆内的样本数小于阈值，则放弃该圆的绘制。不断重复以上步骤直到没有可画的圆为止，并将这些圆内的样本点归为一个集群。</p>

<p>下图展示了该算法的原理：</p>

<figure class="mid">
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-DBSCAN-algorithm.jpg" alt="" height="230">
</figure>

<p>注意，以上两图内都存在不属于高密度集群内的点（游离点），因此这也是 DBSCAN 算法的优势之一，即可以使用密度的概念剔除不属于任一类别的噪点。</p>

<h3>代码实现</h3>

<p>DBSCAN 算法的实现与 KMeans 算法遵循同样的接口。它可以调整的参数有 <code>eps</code>（作圆的半径）和 <code>min_samples</code>（圆内的最少样本数），以此来控制密度：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>cluster <span style="color: #39adb5;font-weight: bold;">import</span> DBSCAN</div><div>cluster <span style="color: #7c4dff;">=</span> DBSCAN<span style="color: #39adb5;">(</span><span style="color: #e53935;">eps</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">0.2</span><span style="color: #39adb5;">,</span> <span style="color: #e53935;">min_samples</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">5</span><span style="color: #39adb5;">)</span></div><div>cluster<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">DBSCAN(eps=0.2, min_samples=5)</div>
</div>

<p>聚类的结果也可以通过 <code>.labels_</code> 属性检查。之前说过 DBSCAN 算法可以判断噪点的存在，噪点在 <code>.labels_</code> 结果内对应的标签均为 -1 ，因此可以用类型以下的形式在散点图中标出噪点，效果与上文展示的图片类似：</p>

<div class="vscode-block"><div>plt<span style="color: #39adb5;">.</span>scatter<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">[:,</span> <span style="color: #f76d47;">0</span><span style="color: #39adb5;">][</span>cluster<span style="color: #39adb5;">.</span>labels_ <span style="color: #7c4dff;">==</span> <span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">],</span> X<span style="color: #39adb5;">[:,</span> <span style="color: #f76d47;">1</span><span style="color: #39adb5;">][</span>cluster<span style="color: #39adb5;">.</span>labels_ <span style="color: #7c4dff;">==</span> <span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">],</span> </div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; <span style="color: #e53935;">s</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">30</span><span style="color: #39adb5;">,</span> <span style="color: #e53935;">c</span><span style="color: #7c4dff;">=</span><span token="string">'g'</span><span style="color: #39adb5;">,</span> <span style="color: #e53935;">marker</span><span style="color: #7c4dff;">=</span><span token="string">'x'</span><span style="color: #39adb5;">)</span></div></div>

<p>DBSCAN 的结果还包含了 <code>.components_</code> 属性，可以得到半径内包含足够样本的核心实例本身，或者可以使用 <code>.core_sample_indices_</code> 属性获取它们的索引。</p>

<p>虽然 <code>DBSCAN</code> 的接口遵循 Scikit-Learn 接口的标准，但它没有 <code>.predict()</code> 方法。这是由于聚类算法不一定适合用于分类：有时候预测一个新样本分类的难度可能和重新聚类差别不大，尤其是 DBSCAN 可能出现同时被划分到两个类别且难以判断哪个是更好的情况。因此聚类完成后可以将结果应用到其余的分类器中，例如 KMeans 算法的思路就和 <span class="math">\\( K=1 \\)</span> 的 <a>K 近邻算法</a>很像。</p>

<h3>结果分析</h3>

<p>和 KMeans 一样，DBSCAN 的参数 <code>eps</code> 和 <code>min_samples</code> 也并不好直接确定。若密度选取过小，两个邻近集群可能被合并；若密度选择过大，则一个集群可能会被拆分。</p>

<p>参数 <code>min_samples</code> 可以使用以下的原则确定：</p>

<div class="math">
    \\[
        \text{min points} \geq \text{dim} + 1
    \\]
</div>

<p><code>eps</code> 的值可以使用绘制 K-距离曲线的方式得到。K-距离表示每个样本第 K 个最近邻样本距离。将数据集所有点对应的 K 近邻距离按照降序方式排序，便可以绘制出 K 距离图线。</p>

<p>可以通过以下方式得到所有样本点排序后的 K-距离：</p>

<div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> sklearn<span style="color: #39adb5;">.</span>neighbors <span style="color: #39adb5;font-weight: bold;">import</span> NearestNeighbors</div><br><div>neighbors <span style="color: #7c4dff;">=</span> NearestNeighbors<span style="color: #39adb5;">(</span><span style="color: #e53935;">n_neighbors</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">4</span><span style="color: #39adb5;">)</span></div><div>distances<span style="color: #39adb5;">,</span> indices <span style="color: #7c4dff;">=</span> neighbors<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">).</span>kneighbors<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">)</span></div><br><div>distances <span style="color: #7c4dff;">=</span> np<span style="color: #39adb5;">.</span>sort<span style="color: #39adb5;">(</span>distances<span style="color: #39adb5;">,</span> <span style="color: #e53935;">axis</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">0</span><span style="color: #39adb5;">)[:,</span> <span style="color: #f76d47;">1</span><span style="color: #39adb5;">][::</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">]</span></div></div>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-output-K-distance.png" alt="" width="460">
</figure>

<p>曲线图明显拐点位置为对应较好的参数。在本示例中，<code>eps</code> 可以取 1.0 左右。</p>

<p>最后要注意的是，虽然这种方式比起随机选择的参数具有一定的合理性，但是得到的参数仍然未必是合理的。</p>

<p>下图对比了 KMeans 算法与 DBSCAN 算法对同一批数据的聚类结果。对比结果可以发现：DBSCAN 算法在聚类时可以发现任意形状的簇将其聚为一类，而 KMeans 算法只是机械地将距离近的数据归到同一组：</p>

<figure class="mid">
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-cluster-algorithm-compare.png" alt="" height="170">
    <figcaption>图片来源于网络</figcaption>
</figure>

<p>总的来说，DBSCAN 算法不需要事先知道集群数，并且可以发现任意形状的集群，初始中心点选择也不影响聚类结果。但它的特点决定了不适用于密度变化较大的数据，参数也难以确实最优值。因此如果难以通过可视化等方式检查数据并确定合适的参数的话，尽量还是谨慎选用该算法。</p>

<h2>示例：图像颜色聚合</h2>

<p>接下来通过一个具体的示例介绍聚类算法的使用。这里通过聚类算法分析一张图片的主要颜色组成，将其提取出来并用于压缩图像颜色。</p>

<p>一个图像由若干像素点组成，一般在网络上传输的图像每一个像素点都使用 RGB 色值表示，每一个色值占据一个字节。那么这样的一个图像可以使用形状为 <code>(image_height, image_width, 3)</code> 的数组描述。可以使用以下代码获取这样的数值：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div><span style="color: #39adb5;font-weight: bold;">from</span> PIL <span style="color: #39adb5;font-weight: bold;">import</span> Image</div><div>image <span style="color: #7c4dff;">=</span> np<span style="color: #39adb5;">.</span>array<span style="color: #39adb5;">(</span>Image<span style="color: #39adb5;">.</span>open<span style="color: #39adb5;">(</span><span style="color: #7c4dff;font-weight: bold;">r</span><span style="color: #39adb5;">'images\image-03.jpg'</span><span style="color: #39adb5;">))</span></div></div>
</div>

<p>考虑到这里只关注像素的 RGB 通道，对像素的位置不感兴趣，因此可以将其转为自变量为 RGB 值的二维数据：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>X <span style="color: #7c4dff;">=</span> image<span style="color: #39adb5;">.</span>reshape<span style="color: #39adb5;">(</span><span style="color: #7c4dff;">-</span><span style="color: #f76d47;">1</span><span style="color: #39adb5;">,</span> <span style="color: #f76d47;">3</span><span style="color: #39adb5;">)</span></div></div>
</div>

<p>然后利用以上数据建立聚类模型。由于自变量的密度不好确定且可能改变，并且为了便于使用中心值描述每一类的结果，在已知 K 值的情况下，使用 KMeans 是一个更好的选择：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>model <span style="color: #7c4dff;">=</span> KMeans<span style="color: #39adb5;">(</span><span style="color: #e53935;">n_clusters</span><span style="color: #7c4dff;">=</span><span style="color: #f76d47;">6</span><span style="color: #39adb5;">)</span></div><div>model<span style="color: #39adb5;">.</span>fit<span style="color: #39adb5;">(</span>X<span style="color: #39adb5;">)</span></div><div>colors <span style="color: #7c4dff;">=</span> model<span style="color: #39adb5;">.</span>cluster_centers_ </div><div><span style="color: #6182b8;">print</span><span style="color: #39adb5;">(</span>colors<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">[[130.67128133  83.12713456  79.94169614]
 [  3.56554958  26.66450421  36.41714522]
 [219.65209098 126.73938314  97.28939034]
 [191.28186172  90.94572996  72.60806672]
 [ 67.96981169  62.53358817  67.36300707]
 [241.93716223 157.78285689 119.45559918]]</div>
</div>

<p>以下展示了部分示例图片聚合得到的主要颜色值：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-clusted-colors.jpg" alt="" width="600">
</figure>

<p>可以根据结果，将一批图像通过二次聚类的方式按颜色分类，还可以用于将图像包含的颜色压缩。颜色压缩也很简单，只需要将每一类颜色都填充其中心点的色值即可：</p>

<div class="jupyter codeblock">
    <div class="jupyter-mark-in"></div>
    <div class="vscode-block"><div>segmented_image <span style="color: #7c4dff;">=</span> model<span style="color: #39adb5;">.</span>cluster_centers_<span style="color: #39adb5;">[</span>model<span style="color: #39adb5;">.</span>labels_<span style="color: #39adb5;">]</span> &nbsp; <span style="color: #39adb5;">\</span></div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="color: #39adb5;">.</span>reshape<span style="color: #39adb5;">(</span>image<span style="color: #39adb5;">.</span>shape<span style="color: #39adb5;">)</span> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="color: #39adb5;">\</span></div><div>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<span style="color: #39adb5;">.</span>astype<span style="color: #39adb5;">(</span>np<span style="color: #39adb5;">.</span>uint8<span style="color: #39adb5;">)</span></div><div>plt<span style="color: #39adb5;">.</span>imshow<span style="color: #39adb5;">(</span>segmented_image<span style="color: #39adb5;">)</span></div></div>
    <div class="jupyter-mark-out"></div>
    <div class="jupyter-output">&lt;matplotlib.image.AxesImage at 0x25a800741f0&gt;</div>
</div>

<p>以下对比了同一图像在不同颜色数下图像压缩的结果：</p>

<figure>
    <img decoding="async" src="/wordpress/wp-content/uploads/2022/11/10-segmented-color.jpg" alt="" width="840">
</figure>
<p><a rel="nofollow" href="/archives/851">机器学习-数据聚类与分群</a>最先出现在<a rel="nofollow" href="">冰封残烛的个人小站</a>。</p>
]]></content:encoded>
					
					<wfw:commentRss>/archives/851/feed</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
	</channel>
</rss>
